{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Finding words, phrases, names and concepts\n",
    "This chapter will introduce you to the basics of text processing with spaCy. You'll learn about the data structures, how to work with statistical models, and how to use them to predict linguistic features in your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the English language class from spacy\n",
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __English__ class object includes language-specific rules for tokenization: words, numbers, and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an English NLP object\n",
    "nlp = English()\n",
    "\n",
    "doc = nlp(\"This is an introductory lesson to spaCy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documents, spans, and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree kangaroos\n",
      "tree kangaroos and narwhals\n"
     ]
    }
   ],
   "source": [
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = doc[2:4]\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = doc[2:6]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens and Characters\n",
    "Accessing words in variable `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like tree kangaroos and narwhals.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the entire string using the `.text` attribute.\n",
    "doc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using index notation to access the first token.\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span Object\n",
    "Using index notation on an English/nlp object returns a span object. This is merely a __view__ of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I like tree"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using index notation to view the first 3 tokens of the document.\n",
    "doc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I li'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using index notation with `.text` attribute to access the first 4 characters of the document.\n",
    "doc.text[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liebe Grüße!\n",
      "¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "# Import the Spanish, German classifiers spacy.lang.es, spacy.lang.de\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# German and Spanish\n",
    "nlp_german = German()\n",
    "nlp_spanish = Spanish()\n",
    "\n",
    "# Instantiate german and spanish nlp objects\n",
    "doc_german = nlp_german(\"Liebe Grüße!\")\n",
    "doc_spanish = nlp_spanish(\"¿Cómo estás?\")\n",
    "\n",
    "# Print each word in the doc\n",
    "print(doc_german.text)\n",
    "print(doc_spanish.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4, 5, 6]\n",
      "Index:    ['I', 'like', 'tree', 'kangaroos', 'and', 'narwhals', '.']\n"
     ]
    }
   ],
   "source": [
    "# Print the index of each token in the document.\n",
    "print(\"Index:   \", [token.i for token in doc])\n",
    "\n",
    "# Print each token in the document.\n",
    "print(\"Index:   \", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Conditional Operators\n",
    "- doc`.is_alpha` returns True if the token contains all characters, False otherwise.\n",
    "- doc`.is_punct` returns True if the token is punctuation, False otherwise.\n",
    "- doc`.like_num` returns True if the token is a digit or numeric word, i.e. \"Ten\". False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new `doc` variable for this example.\n",
    "doc_lexi = nlp(\"There are ten houses priced at $10,000,000 dollars. You bought 5. How much did it cost?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_alpha: [True, True, True, True, True, True, False, False, True, False, True, True, False, False, True, True, True, True, True, False]\n"
     ]
    }
   ],
   "source": [
    "# Determine if a token is homogenously composed of alpha characters.\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc_lexi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_punct: [False, False, False, False, False, False, False, False, False, True, False, False, False, True, False, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "# Determine if a token is a punctuation character.\n",
    "print(\"is_punct:\", [token.is_punct for token in doc_lexi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like_num: [False, False, True, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Determine if a token is like a number.\n",
    "print(\"like_num:\", [token.like_num for token in doc_lexi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Example from spaCy docs\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document, view the next token\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            # If a percentage is next to a number like token, return the numeric token.\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Models\n",
    "\n",
    "__`spaCy`__'s statisitcal models allow a user to predict lingustical attributes in _context_.\n",
    "- Part-of-speech tags `token.pos_`\n",
    "- Syntactic dependencies\n",
    "- Named entities\n",
    "\n",
    "These models are trained large datasets on labeled example texts and can be updates with more examples to fine-tune predictions, i.e. your specific data.\n",
    "\n",
    "`en_core_web_sm` is a small English model trained on web text.\n",
    "- Contains binary weights of the model\n",
    "- Vocabulary, language, and pipeline information built into the model.\n",
    "\n",
    "To install the model use the following command in Terminal:\n",
    "\n",
    "```python\n",
    "$ python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "# Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spacy small English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Part-of-speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pineapple NOUN\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "# Example from spacy docs\n",
    "\n",
    "# Process the text using the small English model\n",
    "doc = nlp(\"She ate the pineapple pizza\")\n",
    "\n",
    "# Iterate over each token in the doc object\n",
    "for token in doc:\n",
    "    # For each token print the text and part-of-speech the tag is used.\n",
    "    # Using an attribute without an underscore will return an integer indicating the index.\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Syntactic Dependencies\n",
    "- `.text` returns the text of a token\n",
    "- `.pos_` returns the part of speech a word: Noun, verb, etc.\n",
    "- `dep_` returns the dependancy label of the token.\n",
    "- `.head.text` returns the parent token that the dependency. Shows the word that the token is attached to/dependant on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pineapple NOUN compound pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech tag meaning\n",
    "- nsubj: nominal subject\n",
    "- det: determiner\n",
    "- dobj: direct object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Named Entities\n",
    "Named entities are real world objects that are assigned a name, i.e. Apple\n",
    "\n",
    "__`spaCy`__ allows you to access named entities from a doc by using the `.ents` attibute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# Process the text through the simple English model\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Iterate over the entities in the doc object\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy.explain method\n",
    "\n",
    "The `spacy.explain` method allows a user to get quick definitions of the most common tags and labels.\n",
    "\n",
    "Docstring: Get a description for a given POS tag, dependency label or entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Geopolitical entities\n",
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'compound'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('compound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'direct object'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting linguistic annotations\n",
    "\n",
    "Display the Part-of-speech tag for each token with its word dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It          PRON      nsubj     \n",
      "’s          VERB      ccomp     \n",
      "official    ADJ       acomp     \n",
      ":           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "is          AUX       ROOT      \n",
      "the         DET       det       \n",
      "first       ADJ       amod      \n",
      "U.S.        PROPN     nmod      \n",
      "public      ADJ       amod      \n",
      "company     NOUN      attr      \n",
      "to          PART      aux       \n",
      "reach       VERB      relcl     \n",
      "a           DET       det       \n",
      "$           SYM       quantmod  \n",
      "1           NUM       compound  \n",
      "trillion    NUM       nummod    \n",
      "market      NOUN      compound  \n",
      "value       NOUN      dobj      \n"
     ]
    }
   ],
   "source": [
    "# Load spacy's Simple English Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Text to pass into the nlp object\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # Display the information for each token.\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of __Auxiliary__<br>\n",
    ">\"in grammar, a helping element, typically a verb, that adds meaning to the basic meaning of the main verb in a clause. Auxiliaries can convey information about tense, mood, person, and number. An auxiliary verb occurs with a main verb that is in the form of an infinitive or a participle.\n",
    ">\n",
    ">English has a rich system of auxiliaries. English auxiliary verbs include the modal verbs, which may express such notions as possibility (“may,” “might,” “can,” “could”) or necessity (“must”). In “Sam should write to his mother,” the modal verb “should” adds the sense of obligation to the main verb “write.” Other English auxiliaries are “will” and “shall,” which often indicate futurity, among other meanings, and “would,” which usually indicates desire or intent. Auxiliaries also help form the passive voice.\" - [Source](https://www.britannica.com/topic/auxiliary)\n",
    "\n",
    "Display the information for each entity in the `doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple       : ORG      - Companies, agencies, institutions, etc.\n",
      "first       : ORDINAL  - \"first\", \"second\", etc.\n",
      "U.S.        : GPE      - Countries, cities, states\n",
      "$1 trillion : MONEY    - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "# Load in spaCy's simple English pre-trained model.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Text to pass into the nlp object\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label. Use spacy.explain to explain the meaning of each label.\n",
    "    print(f\"{ent.text:<12}: {ent.label_:<8} - {spacy.explain(ent.label_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting named entities in context\n",
    "spaCy's models are statistical, yet are not 100% accurate. The accuracy of a pre-trained model depends on the training data and text you're processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Missing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy's simple English pre-trained using web text\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Text to pass into the nlp object\n",
    "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities\n",
    "# Error occurs with iPhone X entity. Answer submitted on spacy.io is correct.\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Get the span for \"iPhone X\"\n",
    "iphone_x = doc[1:3]\n",
    "\n",
    "# Print the span text\n",
    "print(\"Missing entity:\", iphone_x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based Matching\n",
    "spaCy's __Matcher__ let's you write rules to finds word's or phrases in text.\n",
    "\n",
    "Advantages over regular expressions:\n",
    "- spaCy's Matcher works on `doc` objects, not just strings\n",
    "- Match on tokens and token attributes\n",
    "- Use the model's predictions to write a rule based matcher\n",
    "    - Example: \"duck\" (verb) vs. \"duck\" (noun). Find the word \"duck\" only if it's a noun.\n",
    "    \n",
    "## Match Patterns\n",
    "- __List of dictionaries__, one per token\n",
    "    - The keys are the names of token attributes mapped to their expected value.\n",
    "- Match exact token texts\n",
    "```python\n",
    "# Matches case-sensitive phrase \"iPhone X\"\n",
    "[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "```\n",
    "- Match lexical attributes\n",
    "```python\n",
    "# Matches the case-insensitive phrase \"iphone x\"\n",
    "[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "```\n",
    "- Match any token attributes\n",
    "```python\n",
    "# Matches phrases like \"buying milk\" or \"bought flowers\"\n",
    "[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]\n",
    "```\n",
    "\n",
    "## Using spaCy's Matcher\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher attribute from spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load spacy Simple English pre-trained web text model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab of the MODEL\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher `brain/memory`\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": 'X'}]\n",
    "\n",
    "# Add the pattern to the matcher object, pass None to return nothing, pass pattern to store the pattern.\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
    "\n",
    "# Create a doc\n",
    "doc = nlp(\"Upcoming iPhone X release is over-priced and people are over-hyped.\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "Calling the __matcher__ on a doc returns a __list of tuples__. Each tuple contains 3 values.\n",
    "- `match_id`: hash value of the pattern name\n",
    "- `start`: the start index of matched span\n",
    "- `end`: end index of the matched span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "# Continuation from Part 1\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    # Use the start and end index to capture the matching span from the original doc.\n",
    "    matched_span = doc[start_index:end_index]\n",
    "    # Display the matching span\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching lexical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "# Matching a span using a more complex matcher pattern\n",
    "\n",
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "\n",
    "matcher.add(\"FIFA_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "fifa_matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in fifa_matches:\n",
    "    match = doc[start:end]\n",
    "    print(match.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching other token attributes\n",
    "Within a dictionary you can combine root words with Part-of-speech tags to create powerful patterns.\n",
    "\n",
    "In this example the root word of love combined with the verb Part-of speech tag allows us to match patterns that  include \"love\" and \"loved\". The next dictionary in the pattern matches tokens whose Part-of-speech in a noun.\n",
    "- \"loved dogs\"\n",
    "- \"love cats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loved dogs\n",
      "love cats\n"
     ]
    }
   ],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "matcher.add('LOVED_PATTERN', [pattern])\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats.\")\n",
    "\n",
    "love_matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in love_matches:\n",
    "    match = doc[start:end]\n",
    "    print(match.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching using operators and quantifiers\n",
    "### Part 1\n",
    "spaCy's Matcher accepts patterns that contains quantifiers like regular expressions. *, +, ?\n",
    "\n",
    "The example below highlights spaCy's ability to match patterns using optional operators/quantifiers.\n",
    "- \"OP\": operator\n",
    "- \"?\": Match 0 or 1 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "# Pattern using operators and quantifiers\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "matcher.add(\"BUY_PATTERN\", [pattern])\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    match = doc[start:end]\n",
    "    print(match.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using operators and quantifiers\n",
    "### Part 2\n",
    "\n",
    "| Example     | Description             |\n",
    "| :--------   | :---------------------- |\n",
    "|{\"OP\": \"!\"}  | Negation: match 0 times |\n",
    "|{\"OP\": \"?\"}  | Optional: 0 or 1 times  |\n",
    "|{\"OP\": \"+\"}  | Match 1 or more times   |\n",
    "|{\"OP\": \"\\*\"} | Match 0 or more times   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing match patterns\n",
    "### Part 1\n",
    "Write one pattern that only matches mentions of the full iOS versions: “iOS 7”, “iOS 11” and “iOS 10”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "# Create an nlp object using spaCy's simple English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a matcher object with the common words between Matcher and the nlp object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Create a doc\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "Write one pattern that only matches forms of “download” (tokens with the lemma “download”), followed by a token with the part-of-speech tag \"PROPN\" (proper noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Match found: download Winzip\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "Write one pattern that matches adjectives (\"ADJ\") followed by one or two \"NOUN\"s (one noun and one optional noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
