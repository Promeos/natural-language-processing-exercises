{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Processing Pipelines\n",
    "\n",
    "This chapter will show you everything you need to know about spaCy's processing pipeline. You'll learn what goes on under the hood when you process a text, how to write your own components and add them to the pipeline, and how to use custom attributes to add your own metadata to the documents, spans and tokens.\n",
    "\n",
    "![spaCy pipeline](spacy_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the first step of spaCy's pipeline, we need to pass text into a nlp object.\n",
    "    - Words, Sentences, __Text__\n",
    "2. Inside of the `nlp` object, the __tokenizer__ is applied to turn the string of text into a `Doc` object.\n",
    "3. Then the __tagger__, __parser__, and __ner__ (Entity recognizer) process the `Doc` object.\n",
    "4. Finally, a `Doc` object is returned.\n",
    "\n",
    "### Built-in pipeline components\n",
    "\n",
    "| __Name__    | __Description__        | __Creates__                                       |\n",
    "| :---------  | :--------------------- | :------------------------------------------------ |\n",
    "| __tagger__  | Part-of-speech tagger  | Token.tag, Token.pos                              |\n",
    "| __parser__  | Dependency parser      | Token.dep, Token.head, Doc.sents, Doc.noun_chunks |\n",
    "| __ner__     | Named Entity recgnizer | Doc.ents, Token.ent_iob, Token.ent_type           |\n",
    "| __textcat__ | Text classifier        | Doc.cats                                          |\n",
    "\n",
    "---\n",
    "### tagger\n",
    "The Part-of-speech tagger sets the `tag` attribute with the `POS` category the word/token belongs to:\n",
    "\n",
    "#### Alphabetical listing\n",
    "\n",
    "| POS   | Description              | Examples                                     |\n",
    "| :---- | :----------------------- | :------------------------------------------- |\n",
    "| ADJ   | adjective                | big, old, green, incomprehensible, first     |\n",
    "| ADP   | adposition               | in, to, during                               |\n",
    "| ADV   | adverb                   | very, tomorrow, down, where, there           |\n",
    "| AUX\t| auxiliary                | is, has (done), will (do), should (do)       |\n",
    "| CONJ  | conjunction              | and, or, but                                 |\n",
    "| CCONJ | coordinating conjunction | and, or, but                                 |\n",
    "| DET   | determiner\t           | a, an, the                                   |\n",
    "| INTJ  | interjection\t           | psst, ouch, bravo, hello                     |\n",
    "| NOUN  | noun\t                   | girl, cat, tree, air, beauty                 |\n",
    "| NUM   | numeral\t               | 1, 2017, one, seventy-seven, IV, MMXIV       |\n",
    "| PART  | particle\t               | ‚Äôs, not,                                     |\n",
    "| PRON  | pronoun\t               | I, you, he, she, myself, themselves, somebody|\n",
    "| PROPN | proper noun\t           | Mary, John, London, NATO, HBO                |\n",
    "| PUNCT | punctuation\t           | ., (, ), ?                                   |\n",
    "| SCONJ | subordinating conjunction| if, while, that                              |\n",
    "| SYM   | symbol\t               | $, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù            |\n",
    "| VERB  | verb                     | run, runs, running, eat, ate, eating         |\n",
    "| X     | other\t                   | sfpksdpsxmsa                                 |\n",
    "| SPACE | space\t                   | \" \"                                          |\n",
    "\n",
    "---\n",
    "### parser\n",
    "Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "\n",
    "### Universal Dependencies\n",
    "|      | ¬†                                            |\n",
    "| :--- | :------------------------------------------- |\n",
    "| acl  | clausal modifier of noun (adjectival clause) |\n",
    "| advcl | adverbial clause modifier\n",
    "| advmod | adverbial modifier\n",
    "| amod | adjectival modifier\n",
    "| appos | appositional modifier\n",
    "| aux | auxiliary\n",
    "| case | case marking\n",
    "| cc | coordinating conjunction\n",
    "| ccomp | clausal complement\n",
    "| clf | classifier\n",
    "| compound | compound\n",
    "| conj | conjunct\n",
    "| cop | copula\n",
    "| csubj | clausal subject\n",
    "| dep | unspecified dependency\n",
    "| det | determiner\n",
    "| discourse | discourse element\n",
    "| dislocated | dislocated elements\n",
    "| expl | expletive\n",
    "| fixed | fixed multiword expression\n",
    "| flat | flat multiword expression\n",
    "| goeswith | goes with\n",
    "| iobj | indirect object\n",
    "| list | list\n",
    "| mark | marker\n",
    "| nmod | nominal modifier\n",
    "| nsubj | nominal subject\n",
    "| nummod | numeric modifier\n",
    "| obj | object\n",
    "| obl | oblique nominal\n",
    "| orphan | orphan\n",
    "| parataxis | parataxis\n",
    "| punct | punctuation\n",
    "| reparandum | overridden disfluency\n",
    "| root | root\n",
    "| vocative | vocative\n",
    "| xcomp | open clausal complement |\n",
    "\n",
    "### English Dependencies\n",
    "|      |                                             |\n",
    "| :--- | :------------------------------------------ |\n",
    "| acl | clausal modifier of noun (adjectival clause) |\n",
    "| acomp | adjectival complement |\n",
    "| advcl | adverbial clause modifier |\n",
    "| advmod | adverbial modifier |\n",
    "| agent | agent |\n",
    "| amod | adjectival modifier |\n",
    "| appos | appositional modifier |\n",
    "| attr | attribute |\n",
    "| aux | auxiliary |\n",
    "| auxpass | auxiliary (passive) |\n",
    "| case | case marking |\n",
    "| cc | coordinating conjunction |\n",
    "| ccomp | clausal complement |\n",
    "| compound | compound |\n",
    "| conj | conjunct |\n",
    "| cop | copula |\n",
    "| csubj | clausal subject |\n",
    "| csubjpass | clausal subject (passive) |\n",
    "| dative | dative |\n",
    "| dep | unclassified dependent |\n",
    "| det | determiner |\n",
    "| dobj | direct object |\n",
    "| expl | expletive |\n",
    "| intj | interjection |\n",
    "| mark | marker |\n",
    "| meta | meta modifier |\n",
    "| neg | negation modifier |\n",
    "| nn | noun compound modifier |\n",
    "| nounmod | modifier of nominal |\n",
    "| npmod | noun phrase as adverbial modifier |\n",
    "| nsubj | nominal subject |\n",
    "| nsubjpass | nominal subject (passive) |\n",
    "| nummod | numeric modifier |\n",
    "| oprd | object predicate |\n",
    "| obj | object |\n",
    "| obl | oblique nominal |\n",
    "| parataxis | parataxis |\n",
    "| pcomp | complement of preposition |\n",
    "| pobj | object of preposition |\n",
    "| poss | possession modifier |\n",
    "| preconj | pre-correlative conjunction |\n",
    "| prep | prepositional modifier |\n",
    "| prt | particle |\n",
    "| punct | punctuation |\n",
    "| quantmod | modifier of quantifier |\n",
    "| relcl | relative clause modifier |\n",
    "| root | root |\n",
    "| xcomp | open clausal complement |\n",
    "    \n",
    "### ner, Named Entity Recognizer\n",
    "- The __entity recognizer__ adds the _detected_ entities to the `doc.ents` property.\n",
    "- The entity recognizer also sets the entity __type__ attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "\n",
    "### textcat\n",
    "- The text classifier sets category labels that apply __to the whole text__, and adds them to the `doc.cats` property.\n",
    "- __Text categories are very specific. As a result, the text classifier is NOT included in any of the pre-trained models by default. It can be used to train your own systems.__\n",
    "\n",
    "## Under the hood\n",
    "- Pipeline defined in model's `meta.json` in order.\n",
    "    - The metafile defines the language (en, English) and pipeline.\n",
    "    - Tells spaCy which components to instantiate.\n",
    "- Built-in components need binary data to make predictions.\n",
    "    - The binary data used to make predictions is included in the model package. The data is loaded into the component when the model is loaded, `spacy.load(\"en_core_web_lg\")`\n",
    "    \n",
    "# What happens when you call nlp?\n",
    "What does spaCy do when you call nlp on a string of text?\n",
    "\n",
    "```python\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Tokenize the text and apply each pipeline component in order.<br>\n",
    "tokenize -> tagger -> parser -> ner\n",
    "es an input stream into its component tokens.\n",
    "\n",
    "    That's correct!\n",
    "\n",
    "    The tokenizer turns a string of text into a Doc object. spaCy then applies every component in the pipeline on document, in order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the pipeline\n",
    "\n",
    "Let‚Äôs inspect the small English model‚Äôs pipeline!\n",
    "\n",
    "    Load the en_core_web_sm model and create the nlp object.\n",
    "    Print the names of the pipeline components using nlp.pipe_names.\n",
    "    Print the full pipeline of (name, component) tuples using nlp.pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7feaa6e3f9b0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7feaa6e4fef0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7feaa6bdbbb0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7feaa6bdbc90>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7feaa6e94460>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7feaa6da45a0>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Load the en_core_web_lg model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ‚úî Well done! Whenever you're unsure about the current pipeline, you can\n",
    "    inspect it by printing nlp.pipe_names or nlp.pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom pipeline components\n",
    "\n",
    "Custom pipeline components allow a user to add functions to spaCy's pipeline.\n",
    "- Example: Modify a doc and add more data to it.\n",
    "\n",
    "Custom functions execute automaticallly when you call nlp<br>\n",
    "Add your own metadata to documents and tokens<br>\n",
    "Updating built-in attributes like doc.ents<br>\n",
    "- Example: Named Entity Spans\n",
    "\n",
    "## Anatomy of a component (1)\n",
    "- Function that takes a doc, modifies it and returns it.\n",
    "- Functions can be added to the nlp object using `nlp.add_pipe(custom_function)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component(doc):\n",
    "\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a component (2)\n",
    "\n",
    "The reason custom functions added to spaCy's pipeline are called \"components\" is because spaCy's nlp pipeline is made up of a __sequence of components__.\n",
    "- To specify where to add the component in the pipeline, you can use the following keyword arguments:\n",
    "\n",
    "| Argument | Description            | Example                                  |\n",
    "| :------- | :--------------------- | :--------------------------------------- |\n",
    "| last     | If True, add last \t    | nlp.add_pipe(component, last=True)       |\n",
    "| first    | If True, add first \t| nlp.add_pipe(component, first=True)      |\n",
    "| before   | Add before component \t| nlp.add_pipe(component, before=\"ner\")    |\n",
    "| after    | Add after component \t| nlp.add_pipe(component, after=\"tagger\")  |\n",
    "\n",
    "```python\n",
    "nlp.add_pipe(\"custom_component\", [last, first, before, after]=True)\n",
    "```\n",
    "\n",
    "### Example: a simple component (1)\n",
    "Using the new decorator `@Language.component(\"custom_component_name\")` is required in spaCy 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "# The custom component name must be passed as a string.\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: a simple component (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc_length', 'tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"doc_length\")\n",
    "def doc_length(doc):\n",
    "\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"doc_length\", first=True)\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use cases for custom components\n",
    "\n",
    "#### Note: Custom components can only modify the Doc\n",
    "#### Note: Custom components are added to the pipeline after the language class is already initialized and after tokenization.\n",
    "\n",
    "Which of these problems can be solved by custom pipeline components? Choose all that apply!\n",
    "\n",
    "1. Updating the pre-trained models and improving their predictions\n",
    "1. Computing your own values based on tokens and their attributes\n",
    "1. Adding named entities, for example based on a dictionary\n",
    "1. Implementing support for an additional language\n",
    "\n",
    "Answer: 2 and 3\n",
    "\n",
    "    That's correct!\n",
    "\n",
    "    Custom components are great for adding custom values to documents, tokens and spans, and customizing the doc.ents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple components\n",
    "\n",
    "The example shows a custom component that prints the number of tokens in a document. Can you complete it?\n",
    "\n",
    "- Complete the component function with the `doc`‚Äôs length.\n",
    "- Add the `length_component` to the existing pipeline as the first component.\n",
    "- Try out the new pipeline and process any text with the `nlp` object ‚Äì for example ‚ÄúThis is a sentence.‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "This document is 11 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "@Language.component(\"length_component\")\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"I've just created my first custom component in spaCy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Components\n",
    "\n",
    "In this exercise, you‚Äôll be writing a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents. A PhraseMatcher with the animal patterns has already been created as the variable matcher.\n",
    "\n",
    "1. Define the custom component and apply the matcher to the `doc`.\n",
    "1. Create a `Span` for each match, assign the label ID for `\"ANIMAL\"` and overwrite the `doc.ents` with the new spans.\n",
    "1. Add the new component to the pipeline after the `\"ner\"` component.\n",
    "1. Process the text and print the entity text and entity label for the entities in `doc.ents`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'animal_component', 'attribute_ruler', 'lemmatizer']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# Load the large spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# A list of animals we want to add to our named entities\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"Animal\", *[animal_patterns])\n",
    "\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ‚úî Good job! You've built your first pipeline component for rule-based\n",
    "    entity matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension Attributes\n",
    "\n",
    "In this lesson, you'll learn how to add custom attributes to the Doc, Token and Span objects to store custom data.\n",
    "\n",
    "## Setting Custom Attributes\n",
    "- Add custom metadata to documents, tokens and spans\n",
    "- Accessible via the `._` property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.set_extension('title', default=True, force=True)\n",
    "doc._.title = \"My Document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My Document'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "cat\n",
      "and\n",
      "a\n",
      "Golden\n",
      "Retriever\n"
     ]
    }
   ],
   "source": [
    "for token in doc: print(token) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing global classes from `spacy.tokens`\n",
    "from spacy.tokens import Doc, Span, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a doc object to learn how to use spaCy's custom extensions.\n",
    "doc = nlp('The sky is blue.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Attribute Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the set extension method to add the attribute \"is_color\"\n",
    "# To the doc. The attribute is default to false\n",
    "Token.set_extension('is_color', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each token in the doc is set to False by default.\n",
    "doc[3]._.is_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the attribute of the Token at index 3, ('blue'), to True\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the `is_color` attribute of the token\n",
    "doc[3]._.is_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Property Extensions\n",
    "- `Token` property extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a getter function.\n",
    "# This function returns the property of the token when called.\n",
    "def get_is_color(token):\n",
    "    colors = ['red', 'green', 'blue', 'orange', 'yellow']\n",
    "    return token.text in colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Token  with a `is_color` property using the getter function\n",
    "Token.set_extension('is_color', getter=get_is_color, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n"
     ]
    }
   ],
   "source": [
    "print(doc[3]._.is_color, '-', doc[3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Span` property extensions\n",
    "- `Span` extensions should always use a getter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Span getter funciton that returns a Bool\n",
    "# If the span of tokens contains a color in the list `colors`\n",
    "def get_has_color(span):\n",
    "    colors = ['red', 'green', 'orange', 'blue', 'yellow']\n",
    "    return any(token.text in colors for token in span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the custom extension on the `Span` object. Assign the `getter` parameter with the function\n",
    "# `get_has_color`\n",
    "Span.set_extension('has_color', getter=get_has_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does > The sky ; contain a color? - False\n"
     ]
    }
   ],
   "source": [
    "print(\"Does >\", doc[:2], '; contain a color? -', doc[:2]._.has_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does > is blue. ; contain a color? - True\n"
     ]
    }
   ],
   "source": [
    "print(\"Does >\", doc[2:], '; contain a color? -', doc[2:]._.has_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Method Extensions\n",
    "- Assign an object that becomes available as an object method\n",
    "- You can pass arguments to the object method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method function\n",
    "\n",
    "def has_token(doc, token_text):\n",
    "    '''\n",
    "    This function accepts a doc object and token_text as a string.\n",
    "    Returns True or False if the string provided to token_text is a token\n",
    "    of the spaCy doc object.\n",
    "    '''\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension(name='has_token', method=has_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing the color blue as an argument to the method `has token`\n",
    "doc._.has_token('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.has_token('cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Extension Attributes (1)\n",
    "## Let‚Äôs practice setting some extension attributes.\n",
    "### Step 1\n",
    "- Use `Token.set_extension` to register \"is_country\" (default False).\n",
    "- Update it for \"Spain\" and print it for all tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False, force=True)\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- Use `Token.set_extension` to register \"reversed\" (getter function `get_reversed`).\n",
    "- Print its value for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Extension Attributes (2)\n",
    "\n",
    "Let‚Äôs try setting some more complex attributes using getters and method extensions.\n",
    "### Part 1\n",
    "- Complete the `get_has_number` function .\n",
    "- Use `Doc.set_extension` to register \"has_number\" (getter `get_has_number`) and print its value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Register the Doc property extension \"has_number\" with the getter get_has_number\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The False\n",
      "museum False\n",
      "closed False\n",
      "for False\n",
      "five True\n",
      "years False\n",
      "in False\n",
      "2012 True\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "for token in doc: print(token.text,token.like_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "- Use `Span.set_extension` to register \"to_html\" (method `to_html`).\n",
    "- Call it on doc[0:2] with the tag \"strong\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Register the Span method extension \"to_html\" with the method to_html\n",
    "Span.set_extension(\"to_html\", method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name \"strong\"\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html('strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entities and extensions\n",
    "In this exercise, you‚Äôll combine custom extension attributes with the model‚Äôs predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location.\n",
    "\n",
    "- Complete the `get_wikipedia_url` getter so it only returns the URL if the span‚Äôs label is in the list of labels.\n",
    "- Set the Span extension \"wikipedia_url\" using the getter `get_wikipedia_url`.\n",
    "- Iterate over the entities in the doc and output their Wikipedia URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url, force=True)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of extensions\n",
    "\n",
    "Extension attributes are especially powerful if they‚Äôre combined with custom pipeline components. In this exercise, you‚Äôll write a pipeline component that finds country names and a custom extension attribute that returns a country‚Äôs capital, if available.\n",
    "\n",
    "A phrase matcher with all countries is available as the variable matcher. A dictionary of countries mapped to their capital cities is available as the variable CAPITALS.\n",
    "\n",
    "- Complete the `countries_component` and create a Span with the label \"GPE\" (geopolitical entity) for all matches.\n",
    "- Add the component to the pipeline.\n",
    "- Register the Span extension attribute \"capital\" with the getter `get_capital`.\n",
    "- Process the text and print the entity text, entity label and entity capital for each entity span in `doc.ents`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n",
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "COUNTRIES = ['Afghanistan',\n",
    " '√Öland Islands',\n",
    " 'Albania',\n",
    " 'Algeria',\n",
    " 'American Samoa',\n",
    " 'Andorra',\n",
    " 'Angola',\n",
    " 'Anguilla',\n",
    " 'Antarctica',\n",
    " 'Antigua and Barbuda',\n",
    " 'Argentina',\n",
    " 'Armenia',\n",
    " 'Aruba',\n",
    " 'Australia',\n",
    " 'Austria',\n",
    " 'Azerbaijan',\n",
    " 'Bahamas',\n",
    " 'Bahrain',\n",
    " 'Bangladesh',\n",
    " 'Barbados',\n",
    " 'Belarus',\n",
    " 'Belgium',\n",
    " 'Belize',\n",
    " 'Benin',\n",
    " 'Bermuda',\n",
    " 'Bhutan',\n",
    " 'Bolivia (Plurinational State of)',\n",
    " 'Bonaire, Sint Eustatius and Saba',\n",
    " 'Bosnia and Herzegovina',\n",
    " 'Botswana',\n",
    " 'Bouvet Island',\n",
    " 'Brazil',\n",
    " 'British Indian Ocean Territory',\n",
    " 'United States Minor Outlying Islands',\n",
    " 'Virgin Islands (British)',\n",
    " 'Virgin Islands (U.S.)',\n",
    " 'Brunei Darussalam',\n",
    " 'Bulgaria',\n",
    " 'Burkina Faso',\n",
    " 'Burundi',\n",
    " 'Cambodia',\n",
    " 'Cameroon',\n",
    " 'Canada',\n",
    " 'Cabo Verde',\n",
    " 'Cayman Islands',\n",
    " 'Central African Republic',\n",
    " 'Chad',\n",
    " 'Chile',\n",
    " 'China',\n",
    " 'Christmas Island',\n",
    " 'Cocos (Keeling) Islands',\n",
    " 'Colombia',\n",
    " 'Comoros',\n",
    " 'Congo',\n",
    " 'Congo (Democratic Republic of the)',\n",
    " 'Cook Islands',\n",
    " 'Costa Rica',\n",
    " 'Croatia',\n",
    " 'Cuba',\n",
    " 'Cura√ßao',\n",
    " 'Cyprus',\n",
    " 'Czech Republic',\n",
    " 'Denmark',\n",
    " 'Djibouti',\n",
    " 'Dominica',\n",
    " 'Dominican Republic',\n",
    " 'Ecuador',\n",
    " 'Egypt',\n",
    " 'El Salvador',\n",
    " 'Equatorial Guinea',\n",
    " 'Eritrea',\n",
    " 'Estonia',\n",
    " 'Ethiopia',\n",
    " 'Falkland Islands (Malvinas)',\n",
    " 'Faroe Islands',\n",
    " 'Fiji',\n",
    " 'Finland',\n",
    " 'France',\n",
    " 'French Guiana',\n",
    " 'French Polynesia',\n",
    " 'French Southern Territories',\n",
    " 'Gabon',\n",
    " 'Gambia',\n",
    " 'Georgia',\n",
    " 'Germany',\n",
    " 'Ghana',\n",
    " 'Gibraltar',\n",
    " 'Greece',\n",
    " 'Greenland',\n",
    " 'Grenada',\n",
    " 'Guadeloupe',\n",
    " 'Guam',\n",
    " 'Guatemala',\n",
    " 'Guernsey',\n",
    " 'Guinea',\n",
    " 'Guinea-Bissau',\n",
    " 'Guyana',\n",
    " 'Haiti',\n",
    " 'Heard Island and McDonald Islands',\n",
    " 'Holy See',\n",
    " 'Honduras',\n",
    " 'Hong Kong',\n",
    " 'Hungary',\n",
    " 'Iceland',\n",
    " 'India',\n",
    " 'Indonesia',\n",
    " \"C√¥te d'Ivoire\",\n",
    " 'Iran (Islamic Republic of)',\n",
    " 'Iraq',\n",
    " 'Ireland',\n",
    " 'Isle of Man',\n",
    " 'Israel',\n",
    " 'Italy',\n",
    " 'Jamaica',\n",
    " 'Japan',\n",
    " 'Jersey',\n",
    " 'Jordan',\n",
    " 'Kazakhstan',\n",
    " 'Kenya',\n",
    " 'Kiribati',\n",
    " 'Kuwait',\n",
    " 'Kyrgyzstan',\n",
    " \"Lao People's Democratic Republic\",\n",
    " 'Latvia',\n",
    " 'Lebanon',\n",
    " 'Lesotho',\n",
    " 'Liberia',\n",
    " 'Libya',\n",
    " 'Liechtenstein',\n",
    " 'Lithuania',\n",
    " 'Luxembourg',\n",
    " 'Macao',\n",
    " 'Macedonia (the former Yugoslav Republic of)',\n",
    " 'Madagascar',\n",
    " 'Malawi',\n",
    " 'Malaysia',\n",
    " 'Maldives',\n",
    " 'Mali',\n",
    " 'Malta',\n",
    " 'Marshall Islands',\n",
    " 'Martinique',\n",
    " 'Mauritania',\n",
    " 'Mauritius',\n",
    " 'Mayotte',\n",
    " 'Mexico',\n",
    " 'Micronesia (Federated States of)',\n",
    " 'Moldova (Republic of)',\n",
    " 'Monaco',\n",
    " 'Mongolia',\n",
    " 'Montenegro',\n",
    " 'Montserrat',\n",
    " 'Morocco',\n",
    " 'Mozambique',\n",
    " 'Myanmar',\n",
    " 'Namibia',\n",
    " 'Nauru',\n",
    " 'Nepal',\n",
    " 'Netherlands',\n",
    " 'New Caledonia',\n",
    " 'New Zealand',\n",
    " 'Nicaragua',\n",
    " 'Niger',\n",
    " 'Nigeria',\n",
    " 'Niue',\n",
    " 'Norfolk Island',\n",
    " \"Korea (Democratic People's Republic of)\",\n",
    " 'Northern Mariana Islands',\n",
    " 'Norway',\n",
    " 'Oman',\n",
    " 'Pakistan',\n",
    " 'Palau',\n",
    " 'Palestine, State of',\n",
    " 'Panama',\n",
    " 'Papua New Guinea',\n",
    " 'Paraguay',\n",
    " 'Peru',\n",
    " 'Philippines',\n",
    " 'Pitcairn',\n",
    " 'Poland',\n",
    " 'Portugal',\n",
    " 'Puerto Rico',\n",
    " 'Qatar',\n",
    " 'Republic of Kosovo',\n",
    " 'R√©union',\n",
    " 'Romania',\n",
    " 'Russian Federation',\n",
    " 'Rwanda',\n",
    " 'Saint Barth√©lemy',\n",
    " 'Saint Helena, Ascension and Tristan da Cunha',\n",
    " 'Saint Kitts and Nevis',\n",
    " 'Saint Lucia',\n",
    " 'Saint Martin (French part)',\n",
    " 'Saint Pierre and Miquelon',\n",
    " 'Saint Vincent and the Grenadines',\n",
    " 'Samoa',\n",
    " 'San Marino',\n",
    " 'Sao Tome and Principe',\n",
    " 'Saudi Arabia',\n",
    " 'Senegal',\n",
    " 'Serbia',\n",
    " 'Seychelles',\n",
    " 'Sierra Leone',\n",
    " 'Singapore',\n",
    " 'Sint Maarten (Dutch part)',\n",
    " 'Slovakia',\n",
    " 'Slovenia',\n",
    " 'Solomon Islands',\n",
    " 'Somalia',\n",
    " 'South Africa',\n",
    " 'South Georgia and the South Sandwich Islands',\n",
    " 'Korea (Republic of)',\n",
    " 'South Sudan',\n",
    " 'Spain',\n",
    " 'Sri Lanka',\n",
    " 'Sudan',\n",
    " 'Suriname',\n",
    " 'Svalbard and Jan Mayen',\n",
    " 'Swaziland',\n",
    " 'Sweden',\n",
    " 'Switzerland',\n",
    " 'Syrian Arab Republic',\n",
    " 'Taiwan',\n",
    " 'Tajikistan',\n",
    " 'Tanzania, United Republic of',\n",
    " 'Thailand',\n",
    " 'Timor-Leste',\n",
    " 'Togo',\n",
    " 'Tokelau',\n",
    " 'Tonga',\n",
    " 'Trinidad and Tobago',\n",
    " 'Tunisia',\n",
    " 'Turkey',\n",
    " 'Turkmenistan',\n",
    " 'Turks and Caicos Islands',\n",
    " 'Tuvalu',\n",
    " 'Uganda',\n",
    " 'Ukraine',\n",
    " 'United Arab Emirates',\n",
    " 'United Kingdom of Great Britain and Northern Ireland',\n",
    " 'United States of America',\n",
    " 'Uruguay',\n",
    " 'Uzbekistan',\n",
    " 'Vanuatu',\n",
    " 'Venezuela (Bolivarian Republic of)',\n",
    " 'Viet Nam',\n",
    " 'Wallis and Futuna',\n",
    " 'Western Sahara',\n",
    " 'Yemen',\n",
    " 'Zambia',\n",
    " 'Zimbabwe']\n",
    "\n",
    "CAPITALS = {'Afghanistan': 'Kabul',\n",
    " '√Öland Islands': 'Mariehamn',\n",
    " 'Albania': 'Tirana',\n",
    " 'Algeria': 'Algiers',\n",
    " 'American Samoa': 'Pago Pago',\n",
    " 'Andorra': 'Andorra la Vella',\n",
    " 'Angola': 'Luanda',\n",
    " 'Anguilla': 'The Valley',\n",
    " 'Antarctica': '',\n",
    " 'Antigua and Barbuda': \"Saint John's\",\n",
    " 'Argentina': 'Buenos Aires',\n",
    " 'Armenia': 'Yerevan',\n",
    " 'Aruba': 'Oranjestad',\n",
    " 'Australia': 'Canberra',\n",
    " 'Austria': 'Vienna',\n",
    " 'Azerbaijan': 'Baku',\n",
    " 'Bahamas': 'Nassau',\n",
    " 'Bahrain': 'Manama',\n",
    " 'Bangladesh': 'Dhaka',\n",
    " 'Barbados': 'Bridgetown',\n",
    " 'Belarus': 'Minsk',\n",
    " 'Belgium': 'Brussels',\n",
    " 'Belize': 'Belmopan',\n",
    " 'Benin': 'Porto-Novo',\n",
    " 'Bermuda': 'Hamilton',\n",
    " 'Bhutan': 'Thimphu',\n",
    " 'Bolivia (Plurinational State of)': 'Sucre',\n",
    " 'Bonaire, Sint Eustatius and Saba': 'Kralendijk',\n",
    " 'Bosnia and Herzegovina': 'Sarajevo',\n",
    " 'Botswana': 'Gaborone',\n",
    " 'Bouvet Island': '',\n",
    " 'Brazil': 'Bras√≠lia',\n",
    " 'British Indian Ocean Territory': 'Diego Garcia',\n",
    " 'United States Minor Outlying Islands': '',\n",
    " 'Virgin Islands (British)': 'Road Town',\n",
    " 'Virgin Islands (U.S.)': 'Charlotte Amalie',\n",
    " 'Brunei Darussalam': 'Bandar Seri Begawan',\n",
    " 'Bulgaria': 'Sofia',\n",
    " 'Burkina Faso': 'Ouagadougou',\n",
    " 'Burundi': 'Bujumbura',\n",
    " 'Cambodia': 'Phnom Penh',\n",
    " 'Cameroon': 'Yaound√©',\n",
    " 'Canada': 'Ottawa',\n",
    " 'Cabo Verde': 'Praia',\n",
    " 'Cayman Islands': 'George Town',\n",
    " 'Central African Republic': 'Bangui',\n",
    " 'Chad': \"N'Djamena\",\n",
    " 'Chile': 'Santiago',\n",
    " 'China': 'Beijing',\n",
    " 'Christmas Island': 'Flying Fish Cove',\n",
    " 'Cocos (Keeling) Islands': 'West Island',\n",
    " 'Colombia': 'Bogot√°',\n",
    " 'Comoros': 'Moroni',\n",
    " 'Congo': 'Brazzaville',\n",
    " 'Congo (Democratic Republic of the)': 'Kinshasa',\n",
    " 'Cook Islands': 'Avarua',\n",
    " 'Costa Rica': 'San Jos√©',\n",
    " 'Croatia': 'Zagreb',\n",
    " 'Cuba': 'Havana',\n",
    " 'Cura√ßao': 'Willemstad',\n",
    " 'Cyprus': 'Nicosia',\n",
    " 'Czech Republic': 'Prague',\n",
    " 'Denmark': 'Copenhagen',\n",
    " 'Djibouti': 'Djibouti',\n",
    " 'Dominica': 'Roseau',\n",
    " 'Dominican Republic': 'Santo Domingo',\n",
    " 'Ecuador': 'Quito',\n",
    " 'Egypt': 'Cairo',\n",
    " 'El Salvador': 'San Salvador',\n",
    " 'Equatorial Guinea': 'Malabo',\n",
    " 'Eritrea': 'Asmara',\n",
    " 'Estonia': 'Tallinn',\n",
    " 'Ethiopia': 'Addis Ababa',\n",
    " 'Falkland Islands (Malvinas)': 'Stanley',\n",
    " 'Faroe Islands': 'T√≥rshavn',\n",
    " 'Fiji': 'Suva',\n",
    " 'Finland': 'Helsinki',\n",
    " 'France': 'Paris',\n",
    " 'French Guiana': 'Cayenne',\n",
    " 'French Polynesia': 'Papeetƒì',\n",
    " 'French Southern Territories': 'Port-aux-Fran√ßais',\n",
    " 'Gabon': 'Libreville',\n",
    " 'Gambia': 'Banjul',\n",
    " 'Georgia': 'Tbilisi',\n",
    " 'Germany': 'Berlin',\n",
    " 'Ghana': 'Accra',\n",
    " 'Gibraltar': 'Gibraltar',\n",
    " 'Greece': 'Athens',\n",
    " 'Greenland': 'Nuuk',\n",
    " 'Grenada': \"St. George's\",\n",
    " 'Guadeloupe': 'Basse-Terre',\n",
    " 'Guam': 'Hag√•t√±a',\n",
    " 'Guatemala': 'Guatemala City',\n",
    " 'Guernsey': 'St. Peter Port',\n",
    " 'Guinea': 'Conakry',\n",
    " 'Guinea-Bissau': 'Bissau',\n",
    " 'Guyana': 'Georgetown',\n",
    " 'Haiti': 'Port-au-Prince',\n",
    " 'Heard Island and McDonald Islands': '',\n",
    " 'Holy See': 'Rome',\n",
    " 'Honduras': 'Tegucigalpa',\n",
    " 'Hong Kong': 'City of Victoria',\n",
    " 'Hungary': 'Budapest',\n",
    " 'Iceland': 'Reykjav√≠k',\n",
    " 'India': 'New Delhi',\n",
    " 'Indonesia': 'Jakarta',\n",
    " \"C√¥te d'Ivoire\": 'Yamoussoukro',\n",
    " 'Iran (Islamic Republic of)': 'Tehran',\n",
    " 'Iraq': 'Baghdad',\n",
    " 'Ireland': 'Dublin',\n",
    " 'Isle of Man': 'Douglas',\n",
    " 'Israel': 'Jerusalem',\n",
    " 'Italy': 'Rome',\n",
    " 'Jamaica': 'Kingston',\n",
    " 'Japan': 'Tokyo',\n",
    " 'Jersey': 'Saint Helier',\n",
    " 'Jordan': 'Amman',\n",
    " 'Kazakhstan': 'Astana',\n",
    " 'Kenya': 'Nairobi',\n",
    " 'Kiribati': 'South Tarawa',\n",
    " 'Kuwait': 'Kuwait City',\n",
    " 'Kyrgyzstan': 'Bishkek',\n",
    " \"Lao People's Democratic Republic\": 'Vientiane',\n",
    " 'Latvia': 'Riga',\n",
    " 'Lebanon': 'Beirut',\n",
    " 'Lesotho': 'Maseru',\n",
    " 'Liberia': 'Monrovia',\n",
    " 'Libya': 'Tripoli',\n",
    " 'Liechtenstein': 'Vaduz',\n",
    " 'Lithuania': 'Vilnius',\n",
    " 'Luxembourg': 'Luxembourg',\n",
    " 'Macao': '',\n",
    " 'Macedonia (the former Yugoslav Republic of)': 'Skopje',\n",
    " 'Madagascar': 'Antananarivo',\n",
    " 'Malawi': 'Lilongwe',\n",
    " 'Malaysia': 'Kuala Lumpur',\n",
    " 'Maldives': 'Mal√©',\n",
    " 'Mali': 'Bamako',\n",
    " 'Malta': 'Valletta',\n",
    " 'Marshall Islands': 'Majuro',\n",
    " 'Martinique': 'Fort-de-France',\n",
    " 'Mauritania': 'Nouakchott',\n",
    " 'Mauritius': 'Port Louis',\n",
    " 'Mayotte': 'Mamoudzou',\n",
    " 'Mexico': 'Mexico City',\n",
    " 'Micronesia (Federated States of)': 'Palikir',\n",
    " 'Moldova (Republic of)': 'Chi»ôinƒÉu',\n",
    " 'Monaco': 'Monaco',\n",
    " 'Mongolia': 'Ulan Bator',\n",
    " 'Montenegro': 'Podgorica',\n",
    " 'Montserrat': 'Plymouth',\n",
    " 'Morocco': 'Rabat',\n",
    " 'Mozambique': 'Maputo',\n",
    " 'Myanmar': 'Naypyidaw',\n",
    " 'Namibia': 'Windhoek',\n",
    " 'Nauru': 'Yaren',\n",
    " 'Nepal': 'Kathmandu',\n",
    " 'Netherlands': 'Amsterdam',\n",
    " 'New Caledonia': 'Noum√©a',\n",
    " 'New Zealand': 'Wellington',\n",
    " 'Nicaragua': 'Managua',\n",
    " 'Niger': 'Niamey',\n",
    " 'Nigeria': 'Abuja',\n",
    " 'Niue': 'Alofi',\n",
    " 'Norfolk Island': 'Kingston',\n",
    " \"Korea (Democratic People's Republic of)\": 'Pyongyang',\n",
    " 'Northern Mariana Islands': 'Saipan',\n",
    " 'Norway': 'Oslo',\n",
    " 'Oman': 'Muscat',\n",
    " 'Pakistan': 'Islamabad',\n",
    " 'Palau': 'Ngerulmud',\n",
    " 'Palestine, State of': 'Ramallah',\n",
    " 'Panama': 'Panama City',\n",
    " 'Papua New Guinea': 'Port Moresby',\n",
    " 'Paraguay': 'Asunci√≥n',\n",
    " 'Peru': 'Lima',\n",
    " 'Philippines': 'Manila',\n",
    " 'Pitcairn': 'Adamstown',\n",
    " 'Poland': 'Warsaw',\n",
    " 'Portugal': 'Lisbon',\n",
    " 'Puerto Rico': 'San Juan',\n",
    " 'Qatar': 'Doha',\n",
    " 'Republic of Kosovo': 'Pristina',\n",
    " 'R√©union': 'Saint-Denis',\n",
    " 'Romania': 'Bucharest',\n",
    " 'Russian Federation': 'Moscow',\n",
    " 'Rwanda': 'Kigali',\n",
    " 'Saint Barth√©lemy': 'Gustavia',\n",
    " 'Saint Helena, Ascension and Tristan da Cunha': 'Jamestown',\n",
    " 'Saint Kitts and Nevis': 'Basseterre',\n",
    " 'Saint Lucia': 'Castries',\n",
    " 'Saint Martin (French part)': 'Marigot',\n",
    " 'Saint Pierre and Miquelon': 'Saint-Pierre',\n",
    " 'Saint Vincent and the Grenadines': 'Kingstown',\n",
    " 'Samoa': 'Apia',\n",
    " 'San Marino': 'City of San Marino',\n",
    " 'Sao Tome and Principe': 'S√£o Tom√©',\n",
    " 'Saudi Arabia': 'Riyadh',\n",
    " 'Senegal': 'Dakar',\n",
    " 'Serbia': 'Belgrade',\n",
    " 'Seychelles': 'Victoria',\n",
    " 'Sierra Leone': 'Freetown',\n",
    " 'Singapore': 'Singapore',\n",
    " 'Sint Maarten (Dutch part)': 'Philipsburg',\n",
    " 'Slovakia': 'Bratislava',\n",
    " 'Slovenia': 'Ljubljana',\n",
    " 'Solomon Islands': 'Honiara',\n",
    " 'Somalia': 'Mogadishu',\n",
    " 'South Africa': 'Pretoria',\n",
    " 'South Georgia and the South Sandwich Islands': 'King Edward Point',\n",
    " 'Korea (Republic of)': 'Seoul',\n",
    " 'South Sudan': 'Juba',\n",
    " 'Spain': 'Madrid',\n",
    " 'Sri Lanka': 'Colombo',\n",
    " 'Sudan': 'Khartoum',\n",
    " 'Suriname': 'Paramaribo',\n",
    " 'Svalbard and Jan Mayen': 'Longyearbyen',\n",
    " 'Swaziland': 'Lobamba',\n",
    " 'Sweden': 'Stockholm',\n",
    " 'Switzerland': 'Bern',\n",
    " 'Syrian Arab Republic': 'Damascus',\n",
    " 'Taiwan': 'Taipei',\n",
    " 'Tajikistan': 'Dushanbe',\n",
    " 'Tanzania, United Republic of': 'Dodoma',\n",
    " 'Thailand': 'Bangkok',\n",
    " 'Timor-Leste': 'Dili',\n",
    " 'Togo': 'Lom√©',\n",
    " 'Tokelau': 'Fakaofo',\n",
    " 'Tonga': \"Nuku'alofa\",\n",
    " 'Trinidad and Tobago': 'Port of Spain',\n",
    " 'Tunisia': 'Tunis',\n",
    " 'Turkey': 'Ankara',\n",
    " 'Turkmenistan': 'Ashgabat',\n",
    " 'Turks and Caicos Islands': 'Cockburn Town',\n",
    " 'Tuvalu': 'Funafuti',\n",
    " 'Uganda': 'Kampala',\n",
    " 'Ukraine': 'Kiev',\n",
    " 'United Arab Emirates': 'Abu Dhabi',\n",
    " 'United Kingdom of Great Britain and Northern Ireland': 'London',\n",
    " 'United States of America': 'Washington, D.C.',\n",
    " 'Uruguay': 'Montevideo',\n",
    " 'Uzbekistan': 'Tashkent',\n",
    " 'Vanuatu': 'Port Vila',\n",
    " 'Venezuela (Bolivarian Republic of)': 'Caracas',\n",
    " 'Viet Nam': 'Hanoi',\n",
    " 'Wallis and Futuna': 'Mata-Utu',\n",
    " 'Western Sahara': 'El Aai√∫n',\n",
    " 'Yemen': \"Sana'a\",\n",
    " 'Zambia': 'Lusaka',\n",
    " 'Zimbabwe': 'Harare'}\n",
    "\n",
    "# Create an NLP Object\n",
    "nlp = English()\n",
    "\n",
    "# Pass the common Vocabulary to the PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Add the COUNTRY names to the Phrase Matcher\n",
    "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "# To add a compenent to the nlp pipeline, we need to set\n",
    "# The @Language Decorator must be called.\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and performance\n",
    "Processing large volumes of text\n",
    "- Use the nlp.pipe method\n",
    "- Processing text as a stream yields `Doc` objects\n",
    "- Much faster than calling nlp on each text because it batches up the texts\n",
    "\n",
    "#### BAD\n",
    "```python\n",
    "docs = [nlp(text) for text in LOTS_OF_TEXT]\n",
    "```\n",
    "\n",
    "#### GOOD\n",
    "```python\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXT))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing streams\n",
    "\n",
    "In this exercise, you‚Äôll be using `nlp.pipe` for more efficient text processing. The nlp object has already been created for you. A list of tweets about a popular American fast food chain are available as the variable TEXTS.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "- Rewrite the example to use `nlp.pipe`. Instead of iterating over the texts and processing them, iterate over the doc objects yielded by `nlp.pipe`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['open']\n",
      "['terrible', 'payin']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "TEXTS = ['McDonalds is my favorite restaurant.',\n",
    "         'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n",
    "         'People really still eat McDonalds :(',\n",
    "         'The McDonalds in Spain has chicken wings. My heart is so happy ',\n",
    "         '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n",
    "         'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n",
    "         'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']\n",
    "\n",
    "# Process the texts and print the adjectives\n",
    "for text in TEXTS:\n",
    "    doc = nlp(text)\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "- Rewrite the example to use `nlp.pipe`. Don‚Äôt forget to call list() around the result to turn it into a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) () (McDonalds,) (McDonalds, Spain) () () (This morning,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "TEXTS = ['McDonalds is my favorite restaurant.',\n",
    " 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n",
    " 'People really still eat McDonalds :(',\n",
    " 'The McDonalds in Spain has chicken wings. My heart is so happy ',\n",
    " '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n",
    " 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n",
    " 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']\n",
    "\n",
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "Rewrite the example to use `nlp.pipe`. Don‚Äôt forget to call `list()` around the result to turn it into a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[David Bowie, Angela Merkel, Lady Gaga]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))\n",
    "patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
