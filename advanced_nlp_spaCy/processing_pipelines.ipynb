{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Processing Pipelines\n",
    "\n",
    "This chapter will show you everything you need to know about spaCy's processing pipeline. You'll learn what goes on under the hood when you process a text, how to write your own components and add them to the pipeline, and how to use custom attributes to add your own metadata to the documents, spans and tokens.\n",
    "\n",
    "![spaCy pipeline](spacy_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the first step of spaCy's pipeline, we need to pass text into a nlp object.\n",
    "    - Words, Sentences, __Text__\n",
    "2. Inside of the `nlp` object, the __tokenizer__ is applied to turn the string of text into a `Doc` object.\n",
    "3. Then the __tagger__, __parser__, and __ner__ (Entity recognizer) process the `Doc` object.\n",
    "4. Finally, a `Doc` object is returned.\n",
    "\n",
    "### Built-in pipeline components\n",
    "\n",
    "| __Name__    | __Description__        | __Creates__                                       |\n",
    "| :---------  | :--------------------- | :------------------------------------------------ |\n",
    "| __tagger__  | Part-of-speech tagger  | Token.tag, Token.pos                              |\n",
    "| __parser__  | Dependency parser      | Token.dep, Token.head, Doc.sents, Doc.noun_chunks |\n",
    "| __ner__     | Named Entity recgnizer | Doc.ents, Token.ent_iob, Token.ent_type           |\n",
    "| __textcat__ | Text classifier        | Doc.cats                                          |\n",
    "\n",
    "---\n",
    "### tagger\n",
    "The Part-of-speech tagger sets the `tag` attribute with the `POS` category the word/token belongs to:\n",
    "\n",
    "#### Alphabetical listing\n",
    "\n",
    "| POS   | Description              | Examples                                     |\n",
    "| :---- | :----------------------- | :------------------------------------------- |\n",
    "| ADJ   | adjective                | big, old, green, incomprehensible, first     |\n",
    "| ADP   | adposition               | in, to, during                               |\n",
    "| ADV   | adverb                   | very, tomorrow, down, where, there           |\n",
    "| AUX\t| auxiliary                | is, has (done), will (do), should (do)       |\n",
    "| CONJ  | conjunction              | and, or, but                                 |\n",
    "| CCONJ | coordinating conjunction | and, or, but                                 |\n",
    "| DET   | determiner\t           | a, an, the                                   |\n",
    "| INTJ  | interjection\t           | psst, ouch, bravo, hello                     |\n",
    "| NOUN  | noun\t                   | girl, cat, tree, air, beauty                 |\n",
    "| NUM   | numeral\t               | 1, 2017, one, seventy-seven, IV, MMXIV       |\n",
    "| PART  | particle\t               | ‚Äôs, not,                                     |\n",
    "| PRON  | pronoun\t               | I, you, he, she, myself, themselves, somebody|\n",
    "| PROPN | proper noun\t           | Mary, John, London, NATO, HBO                |\n",
    "| PUNCT | punctuation\t           | ., (, ), ?                                   |\n",
    "| SCONJ | subordinating conjunction| if, while, that                              |\n",
    "| SYM   | symbol\t               | $, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù            |\n",
    "| VERB  | verb                     | run, runs, running, eat, ate, eating         |\n",
    "| X     | other\t                   | sfpksdpsxmsa                                 |\n",
    "| SPACE | space\t                   | \" \"                                          |\n",
    "\n",
    "---\n",
    "### parser\n",
    "Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "\n",
    "### Universal Dependencies\n",
    "|      | ¬†                                            |\n",
    "| :--- | :------------------------------------------- |\n",
    "| acl  | clausal modifier of noun (adjectival clause) |\n",
    "| advcl | adverbial clause modifier\n",
    "| advmod | adverbial modifier\n",
    "| amod | adjectival modifier\n",
    "| appos | appositional modifier\n",
    "| aux | auxiliary\n",
    "| case | case marking\n",
    "| cc | coordinating conjunction\n",
    "| ccomp | clausal complement\n",
    "| clf | classifier\n",
    "| compound | compound\n",
    "| conj | conjunct\n",
    "| cop | copula\n",
    "| csubj | clausal subject\n",
    "| dep | unspecified dependency\n",
    "| det | determiner\n",
    "| discourse | discourse element\n",
    "| dislocated | dislocated elements\n",
    "| expl | expletive\n",
    "| fixed | fixed multiword expression\n",
    "| flat | flat multiword expression\n",
    "| goeswith | goes with\n",
    "| iobj | indirect object\n",
    "| list | list\n",
    "| mark | marker\n",
    "| nmod | nominal modifier\n",
    "| nsubj | nominal subject\n",
    "| nummod | numeric modifier\n",
    "| obj | object\n",
    "| obl | oblique nominal\n",
    "| orphan | orphan\n",
    "| parataxis | parataxis\n",
    "| punct | punctuation\n",
    "| reparandum | overridden disfluency\n",
    "| root | root\n",
    "| vocative | vocative\n",
    "| xcomp | open clausal complement |\n",
    "\n",
    "### English Dependencies\n",
    "|      |                                             |\n",
    "| :--- | :------------------------------------------ |\n",
    "| acl | clausal modifier of noun (adjectival clause) |\n",
    "| acomp | adjectival complement |\n",
    "| advcl | adverbial clause modifier |\n",
    "| advmod | adverbial modifier |\n",
    "| agent | agent |\n",
    "| amod | adjectival modifier |\n",
    "| appos | appositional modifier |\n",
    "| attr | attribute |\n",
    "| aux | auxiliary |\n",
    "| auxpass | auxiliary (passive) |\n",
    "| case | case marking |\n",
    "| cc | coordinating conjunction |\n",
    "| ccomp | clausal complement |\n",
    "| compound | compound |\n",
    "| conj | conjunct |\n",
    "| cop | copula |\n",
    "| csubj | clausal subject |\n",
    "| csubjpass | clausal subject (passive) |\n",
    "| dative | dative |\n",
    "| dep | unclassified dependent |\n",
    "| det | determiner |\n",
    "| dobj | direct object |\n",
    "| expl | expletive |\n",
    "| intj | interjection |\n",
    "| mark | marker |\n",
    "| meta | meta modifier |\n",
    "| neg | negation modifier |\n",
    "| nn | noun compound modifier |\n",
    "| nounmod | modifier of nominal |\n",
    "| npmod | noun phrase as adverbial modifier |\n",
    "| nsubj | nominal subject |\n",
    "| nsubjpass | nominal subject (passive) |\n",
    "| nummod | numeric modifier |\n",
    "| oprd | object predicate |\n",
    "| obj | object |\n",
    "| obl | oblique nominal |\n",
    "| parataxis | parataxis |\n",
    "| pcomp | complement of preposition |\n",
    "| pobj | object of preposition |\n",
    "| poss | possession modifier |\n",
    "| preconj | pre-correlative conjunction |\n",
    "| prep | prepositional modifier |\n",
    "| prt | particle |\n",
    "| punct | punctuation |\n",
    "| quantmod | modifier of quantifier |\n",
    "| relcl | relative clause modifier |\n",
    "| root | root |\n",
    "| xcomp | open clausal complement |\n",
    "    \n",
    "### ner, Named Entity Recognizer\n",
    "- The __entity recognizer__ adds the _detected_ entities to the `doc.ents` property.\n",
    "- The entity recognizer also sets the entity __type__ attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "\n",
    "### textcat\n",
    "- The text classifier sets category labels that apply __to the whole text__, and adds them to the `doc.cats` property.\n",
    "- __Text categories are very specific. As a result, the text classifier is NOT included in any of the pre-trained models by default. It can be used to train your own systems.__\n",
    "\n",
    "## Under the hood\n",
    "- Pipeline defined in model's `meta.json` in order.\n",
    "    - The metafile defines the language (en, English) and pipeline.\n",
    "    - Tells spaCy which components to instantiate.\n",
    "- Built-in components need binary data to make predictions.\n",
    "    - The binary data used to make predictions is included in the model package. The data is loaded into the component when the model is loaded, `spacy.load(\"en_core_web_lg\")`\n",
    "    \n",
    "# What happens when you call nlp?\n",
    "What does spaCy do when you call nlp on a string of text?\n",
    "\n",
    "```python\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Tokenize the text and apply each pipeline component in order.<br>\n",
    "tokenize -> tagger -> parser -> ner\n",
    "es an input stream into its component tokens.\n",
    "\n",
    "    That's correct!\n",
    "\n",
    "    The tokenizer turns a string of text into a Doc object. spaCy then applies every component in the pipeline on document, in order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the pipeline\n",
    "\n",
    "Let‚Äôs inspect the small English model‚Äôs pipeline!\n",
    "\n",
    "    Load the en_core_web_sm model and create the nlp object.\n",
    "    Print the names of the pipeline components using nlp.pipe_names.\n",
    "    Print the full pipeline of (name, component) tuples using nlp.pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7fab6161f9b0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7fab6162fef0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7fab613c4bb0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7fab613c4d00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7fab6166eaa0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7fab615935a0>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Load the en_core_web_lg model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ‚úî Well done! Whenever you're unsure about the current pipeline, you can\n",
    "    inspect it by printing nlp.pipe_names or nlp.pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom pipeline components\n",
    "\n",
    "Custom pipeline components allow a user to add functions to spaCy's pipeline.\n",
    "- Example: Modify a doc and add more data to it.\n",
    "\n",
    "Custom functions execute automaticallly when you call nlp<br>\n",
    "Add your own metadata to documents and tokens<br>\n",
    "Updating built-in attributes like doc.ents<br>\n",
    "- Example: Named Entity Spans\n",
    "\n",
    "## Anatomy of a component (1)\n",
    "- Function that takes a doc, modifies it and returns it.\n",
    "- Functions can be added to the nlp object using `nlp.add_pipe(custom_function)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component(doc):\n",
    "\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a component (2)\n",
    "\n",
    "The reason custom functions added to spaCy's pipeline are called \"components\" is because spaCy's nlp pipeline is made up of a __sequence of components__.\n",
    "- To specify where to add the component in the pipeline, you can use the following keyword arguments:\n",
    "\n",
    "| Argument | Description            | Example                                  |\n",
    "| :------- | :--------------------- | :--------------------------------------- |\n",
    "| last     | If True, add last \t    | nlp.add_pipe(component, last=True)       |\n",
    "| first    | If True, add first \t| nlp.add_pipe(component, first=True)      |\n",
    "| before   | Add before component \t| nlp.add_pipe(component, before=\"ner\")    |\n",
    "| after    | Add after component \t| nlp.add_pipe(component, after=\"tagger\")  |\n",
    "\n",
    "```python\n",
    "nlp.add_pipe(\"custom_component\", [last, first, before, after]=True)\n",
    "```\n",
    "\n",
    "### Example: a simple component (1)\n",
    "Using the new decorator `@Language.component(\"custom_component_name\")` is required in spaCy 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "# The custom component name must be passed as a string.\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: a simple component (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc_length', 'tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"doc_length\")\n",
    "def doc_length(doc):\n",
    "\n",
    "    # Print the doc's length\n",
    "    print(\"Doc length:\", len(doc))\n",
    "\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"doc_length\", first=True)\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use cases for custom components\n",
    "\n",
    "#### Note: Custom components can only modify the Doc\n",
    "#### Note: Custom components are added to the pipeline after the language class is already initialized and after tokenization.\n",
    "\n",
    "Which of these problems can be solved by custom pipeline components? Choose all that apply!\n",
    "\n",
    "1. Updating the pre-trained models and improving their predictions\n",
    "1. Computing your own values based on tokens and their attributes\n",
    "1. Adding named entities, for example based on a dictionary\n",
    "1. Implementing support for an additional language\n",
    "\n",
    "Answer: 2 and 3\n",
    "\n",
    "    That's correct!\n",
    "\n",
    "    Custom components are great for adding custom values to documents, tokens and spans, and customizing the doc.ents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple components\n",
    "\n",
    "The example shows a custom component that prints the number of tokens in a document. Can you complete it?\n",
    "\n",
    "- Complete the component function with the `doc`‚Äôs length.\n",
    "- Add the `length_component` to the existing pipeline as the first component.\n",
    "- Try out the new pipeline and process any text with the `nlp` object ‚Äì for example ‚ÄúThis is a sentence.‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "This document is 11 tokens long.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "@Language.component(\"length_component\")\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "doc = nlp(\"I've just created my first custom component in spaCy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Components\n",
    "\n",
    "In this exercise, you‚Äôll be writing a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents. A PhraseMatcher with the animal patterns has already been created as the variable matcher.\n",
    "\n",
    "1. Define the custom component and apply the matcher to the `doc`.\n",
    "1. Create a `Span` for each match, assign the label ID for `\"ANIMAL\"` and overwrite the `doc.ents` with the new spans.\n",
    "1. Add the new component to the pipeline after the `\"ner\"` component.\n",
    "1. Process the text and print the entity text and entity label for the entities in `doc.ents`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'animal_component', 'attribute_ruler', 'lemmatizer']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# Load the large spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# A list of animals we want to add to our named entities\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"Animal\", *[animal_patterns])\n",
    "\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ‚úî Good job! You've built your first pipeline component for rule-based\n",
    "    entity matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension Attributes\n",
    "\n",
    "In this lesson, you'll learn how to add custom attributes to the Doc, Token and Span objects to store custom data.\n",
    "\n",
    "## Setting Custom Attributes\n",
    "- Add custom metadata to documents, tokens and spans\n",
    "- Accessible via the `._` property\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.set_extension('title', default=True, force=True)\n",
    "doc._.title = \"My Document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My Document'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "cat\n",
      "and\n",
      "a\n",
      "Golden\n",
      "Retriever\n"
     ]
    }
   ],
   "source": [
    "for token in doc: print(token) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing global classes from `spacy.tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span, Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension('title', default=None, force=True)\n",
    "Span.set_extension('has_color', default=False, force=True)\n",
    "Token.set_extension('is_color', default = False, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Extensions\n",
    "- Attribute Extensions\n",
    "- Property Extensions\n",
    "- Method Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
