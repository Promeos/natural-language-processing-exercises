{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Large-scale data analysis with spaCy\n",
    "\n",
    "https://course.spacy.io/en/chapter2\n",
    "\n",
    "In this chapter, you'll use your new skills to extract specific information from large volumes of text. You''ll learn how to make the most of spaCy's data structures, and how to effectively combine statistical and rule-based approaches for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures (1)\n",
    "## Vocab, Lexemes, and StringStore\n",
    "\n",
    "### Shared Vocab and StringStore Part 1\n",
    "- __spaCy only communicates in hash ids__\n",
    "- spaCy stores shared strings/tokens/data across multiple documents.\n",
    "- spaCy saves memory by encoding all strings to hash values.\n",
    "- Strings are only stored once in the `StringStore` via `nlp.vocab.strings`\n",
    "- String store: lookup table in both directions.\n",
    "    - Passing a string returns a hash value\n",
    "    ```python\n",
    "    # Hash value\n",
    "    earth_hash = nlp.vocab.strings['Earth']\n",
    "    ```\n",
    "    - Passing a hash value returns a string\n",
    "    ```python\n",
    "    # String value\n",
    "    nlp.vocab.strings[earth_hash]\n",
    "    ```\n",
    "- Hashes cannot be reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10533021089177626446"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "nlp.vocab.strings['Earth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nlp.vocab.strings[hash_value]` will raise an error because the nlp object __has not seen the hash value of Earth__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Earth'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings[10533021089177626446]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Always pass around the shared vocab between a doc and the nlp object__\n",
    "\n",
    "To use the string and hash value as inputs in `nlp.vocab.string[input]` we need to give the nlp object text that contains the word we're trying to look up with it's hash value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Earth'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I live on Earth. It's a beautiful planet with diverse life forms.\" \\\n",
    "          \"Over millions of years, these creatures have adapted to harsh climates.\")\n",
    "\n",
    "# The nlp object has `memory` of the word 'Earth' an successfully returns the string, given its hash value.\n",
    "nlp.vocab.strings[10533021089177626446]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Vocab and String Store Part 2\n",
    "You can use the __nlp object__ and the __doc object__ to look up the string value or hash value of a token.\n",
    "\n",
    "#### Find the string and hash values using the nlp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash value: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love black coffee, from the hearts mountains of Costa Rica.\")\n",
    "\n",
    "# Display the hash value of the string \"coffee\"\n",
    "print(\"Hash value:\", nlp.vocab.strings['coffee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String value: coffee\n"
     ]
    }
   ],
   "source": [
    "# Display the string of the hash value 3197928453018144401\n",
    "print(\"String value:\", nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the string and hash values using the doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash value: 3197928453018144401\n",
      "String value: coffee\n"
     ]
    }
   ],
   "source": [
    "print(\"Hash value:\", doc.vocab.strings['coffee'])\n",
    "print(\"String value:\", doc.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexemes: entries in the vocabulary\n",
    "A `Lexeme` object is an entry in the vocabulary. It contains the __context-independent__ information about a word.\n",
    "- Word text: lexeme.text for the string and lexeme.orth for the hash value\n",
    "- Lexical attributes of the string, e.g. lexeme.is_alpha\n",
    "- Lexemes __DO NOT__ contain Parts-of-speech tags, dependencies, or entity labels.\n",
    ">These attributes depend on the __CONTEXT__ of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Strings to hashes\n",
    "\n",
    "### Part 1\n",
    "\n",
    "    Look up the string “cat” in nlp.vocab.strings to get the hash.\n",
    "    Look up the hash to get back the string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash id of the word 'cat' is 5439657043933447811\n",
      "\n",
      "The string of the hash id 5439657043933447811, is 'cat'\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings['cat']\n",
    "print(f\"The hash id of the word 'cat' is {cat_hash}\", end='\\n\\n')\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(f\"The string of the hash id {cat_hash}, is '{cat_string}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "    Look up the string label “PERSON” in nlp.vocab.strings to get the hash.\n",
    "    Look up the hash to get back the string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash id of the word 'PERSON' is 380\n",
      "\n",
      "The string of the hash id 380, is 'PERSON'\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(f\"The hash id of the word 'PERSON' is {person_hash}\", end='\\n\\n')\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(f\"The string of the hash id {person_hash}, is '{person_string}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Vocab, hashes, and lexemes\n",
    "\n",
    "    Why does this code throw an error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# Create an English and German nlp object\n",
    "nlp = English()\n",
    "nlp_de = German()\n",
    "\n",
    "# Get the ID for the string 'Bowie'\n",
    "bowie_id = nlp.vocab.strings[\"Bowie\"]\n",
    "print(bowie_id)\n",
    "\n",
    "# Look up the ID for \"Bowie\" in the vocab\n",
    "print(nlp_de.vocab.strings[bowie_id])\n",
    "```\n",
    "<br>\n",
    "\n",
    "    Answer: The string \"Bowie\" isn’t in the German vocab, so the hash can’t be resolved in the string store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures (2)\n",
    "## Doc, Span and Token\n",
    "\n",
    "- The __doc__ object is the most important data structure.\n",
    "- The doc object is created automatically whenever text processed by a nlp object\n",
    "\n",
    "### The doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a Doc object manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "doc.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Span object(1)\n",
    "\n",
    "The span is the slice of a Doc consisting of one or more tokens.\n",
    "\n",
    "The __Span__ object takes 3 arguments:\n",
    "- The doc object it refers to.\n",
    "- The start index\n",
    "- The end index, exclusive\n",
    "\n",
    "### The Span object(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello World"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Doc and Span classes from spaCy\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"World\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "Span(doc, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "12946562419758953770\n"
     ]
    }
   ],
   "source": [
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label='GREETING')\n",
    "print(span_with_label)\n",
    "print(span_with_label.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add span to the doc.ents attribute to add an entity to\n",
    "# The list of entities in doc\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Hello World,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "`Doc` and `Span` are very powerful and hold references and relationships of words and sentences.\n",
    "- Convert result to strings as late as possible. Converting strings earlier in the process will cause a loss of all relationships between the tokens.\n",
    "- Use token attributes if available - for example, token.i for the token index.\n",
    "- Always pass shared vocab between the doc and the nlp object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Creating a Doc\n",
    "\n",
    "### Part 1\n",
    "\n",
    "    Import the Doc from spacy.tokens.\n",
    "    Create a Doc from the words and spaces. Don’t forget to pass in the vocab!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "    Import the Doc from spacy.tokens.\n",
    "    Create a Doc from the words and spaces. Don’t forget to pass in the vocab!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "\n",
    "# The bools indicate if a word has a space inbetween the tokens\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "    Import the Doc from spacy.tokens.\n",
    "    Complete the words and spaces to match the desired text and create a doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Docs, spans, entities from scratch\n",
    "\n",
    "In this exercise, you’ll create the `Doc` and `Span` objects manually, and update the named entities – just like spaCy does behind the scenes. A shared nlp object has already been created.\n",
    "\n",
    "    1. Import the `Doc` and `Span` classes from spacy.tokens.\n",
    "    2. Use the `Doc` class directly to create a doc from the words and spaces.\n",
    "    3. Create a `Span` for “David Bowie” from the doc and assign it the label \"PERSON\".\n",
    "    4. Overwrite the `doc.ents` with a list of one entity, the “David Bowie” span.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words, spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Data structures best practices\n",
    "\n",
    "The code in this example is trying to analyze a text and collect all proper nouns that are followed by a verb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "    Why is the code bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "# Get all tokens and part-of-speech tags\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # Check if the current token is a proper noun\n",
    "    if pos == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Answer: It only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships.\n",
    "    \n",
    "    Response: That's correct!\n",
    "\n",
    "    Always convert the results to strings as late as possible, and try to use native token attributes to keep things consistent. \n",
    "\n",
    "### Part 2\n",
    "\n",
    "    Rewrite the code to use the native token attributes instead of lists of `token_texts` and `pos_tags`.\n",
    "    Loop over each token in the doc and check the `token.pos_` attribute.\n",
    "    Use `doc[token.i + 1]` to check for the next token and its `.pos_ attribute`.\n",
    "    If a proper noun before a verb is found, print its `token.text`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ✔ Great work! While the solution here works fine for the given example,\n",
    "    there are still things that can be improved. If the doc ends with a proper noun,\n",
    "    doc[token.i + 1] will fail. To make sure the code generalizes, you should first\n",
    "    check if token.i + 1 < len(doc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\" and (token.i+1 < len(doc)):\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors and semantic similarity\n",
    "\n",
    "### Comparing semantic similarity\n",
    "- spaCy can compare two objects and predict similarity.\n",
    "- `Doc.similarity()`, `Span.similarity()`, `Token.similarity()`\n",
    "- Take a Doc/Span and return a similarity score between 0 and 1.\n",
    "- To compare similarity use a model that has word vectors included:\n",
    "    - `en_core_web_md` or\n",
    "    - `en_core_web_lg`\n",
    "        \n",
    "### Similarity example (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1 and doc2 have 86% similarity.\n"
     ]
    }
   ],
   "source": [
    "# Load a spaCy model with vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "\n",
    "print(f\"doc1 and doc2 have {doc1.similarity(doc2):.0%} similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token1 and token2 have 74% similarity.\n"
     ]
    }
   ],
   "source": [
    "# Comparing two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "\n",
    "print(f\"token1 and token2 have {token1.similarity(token2):.0%} similarity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity example (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc and token have 33% similarity.\n"
     ]
    }
   ],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(f\"doc and token have {doc.similarity(token):.0%} similarity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span and doc have 62% similarity.\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(f\"span and doc have {span.similarity(doc):.0%} similarity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does spaCy predict similarity?\n",
    "\n",
    "- similarity is determined using word vectors\n",
    "- Multi-dimensional meaning representation of words\n",
    "- Generated using an algorithm like Word2Vec and lots of text\n",
    "    - Word2Vec is used to train word vectors from raw text\n",
    "- Can be added to spaCy's statistical models\n",
    "- Default: consine similarity, but can be adjusted\n",
    "- `Doc` and `Span` vectors default to average of token vectors\n",
    "- Short phrases are better than long documents with many irrelevant words\n",
    "\n",
    "### Word vectors in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
      " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
      " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
      "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
      "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
      " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
      "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
      " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
      "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
      "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
      " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
      " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
      "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
      "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
      " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
      "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
      " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
      " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
      "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
      "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
      "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
      "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
      "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
      " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
      " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
      "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
      "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
      "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
      "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
      " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
      "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
      " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
      "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
      " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
      "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
      "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
      " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
      "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
      "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
      " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
      " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
      "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
      " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
      "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
      " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
      " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
      "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
      "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
      "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
      " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
     ]
    }
   ],
   "source": [
    "# Load the larger model with vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "\n",
    "# Access the vector via the token.vector attribute - Vector of banana\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity depends on the application context\n",
    "\n",
    "- Useful for many applications: recommendation systems, flagging duplicates (posts on an online platform) etc.\n",
    "- There's no objective definition of \"similarity\"\n",
    "- Depends on othe context and what the application needs to do\n",
    "\n",
    "In this example, the subject of both docs is 'cat'. The similarity of these two docs is 95%. The both contain the word 'I' and 'cats'. In a sentiment analysis application this similarity would be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9501446702124066\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting word vectors\n",
    "\n",
    "In this exercise, you’ll use a larger English model, which includes around 685.000 word vectors. The model is already pre-installed.\n",
    "\n",
    "    Load the medium \"en_core_web_md\" model with word vectors.\n",
    "    Print the vector for \"bananas\" using the token.vector attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2009e-01 -3.0322e-02 -7.9859e-02 -4.6279e-01 -3.8600e-01  3.6962e-01\n",
      " -7.7178e-01 -1.1529e-01  3.3601e-02  5.6573e-01 -2.4001e-01  4.1833e-01\n",
      "  1.5049e-01  3.5621e-01 -2.1508e-01 -4.2743e-01  8.1400e-02  3.3916e-01\n",
      "  2.1637e-01  1.4792e-01  4.5811e-01  2.0966e-01 -3.5706e-01  2.3800e-01\n",
      "  2.7971e-02 -8.4538e-01  4.1917e-01 -3.9181e-01  4.0434e-04 -1.0662e+00\n",
      "  1.4591e-01  1.4643e-03  5.1277e-01  2.6072e-01  8.3785e-02  3.0340e-01\n",
      "  1.8579e-01  5.9999e-02 -4.0270e-01  5.0888e-01 -1.1358e-01 -2.8854e-01\n",
      " -2.7068e-01  1.1017e-02 -2.2217e-01  6.9076e-01  3.6459e-02  3.0394e-01\n",
      "  5.6989e-02  2.2733e-01 -9.9473e-02  1.5165e-01  1.3540e-01 -2.4965e-01\n",
      "  9.8078e-01 -8.0492e-01  1.9326e-01  3.1128e-01  5.5390e-02 -4.2423e-01\n",
      " -1.4082e-02  1.2708e-01  1.8868e-01  5.9777e-02 -2.2215e-01 -8.3950e-01\n",
      "  9.1987e-02  1.0180e-01 -3.1299e-01  5.5083e-01 -3.0717e-01  4.4201e-01\n",
      "  1.2666e-01  3.7643e-01  3.2333e-01  9.5673e-02  2.5083e-01 -6.4049e-02\n",
      "  4.2143e-01 -1.9375e-01  3.8026e-01  7.0883e-03 -2.0371e-01  1.5402e-01\n",
      " -3.7877e-03 -2.9396e-01  9.6518e-01  2.0068e-01 -5.6572e-01 -2.2581e-01\n",
      "  3.2251e-01 -3.4634e-01  2.7064e-01 -2.0687e-01 -4.7229e-01  3.1704e-01\n",
      " -3.4665e-01 -2.5188e-01 -1.1201e-01 -3.3937e-01  3.1518e-01 -3.2221e-01\n",
      " -2.4530e-01 -7.1571e-02 -4.3971e-01 -1.2070e+00  3.3365e-01 -5.8208e-02\n",
      "  8.0899e-01  4.2335e-01  3.8678e-01 -6.0797e-01 -7.3760e-01 -2.0547e-01\n",
      " -1.7499e-01 -3.7842e-03  2.1930e-01 -5.2486e-02  3.4869e-01  4.3852e-01\n",
      " -3.4471e-01  2.8910e-01  7.2554e-02 -4.8625e-01 -3.8390e-01 -4.4760e-01\n",
      "  4.3278e-01 -2.7128e-03 -9.0067e-01 -3.0819e-02 -3.8630e-01 -8.0798e-02\n",
      " -1.6243e-01  2.8830e-01 -2.6349e-01  1.7628e-01  3.5958e-01  5.7672e-01\n",
      " -5.4624e-01  3.8555e-02 -2.0182e+00  3.2916e-01  3.4672e-01  1.5398e-01\n",
      " -4.3446e-01 -4.1428e-02 -6.9588e-02  5.1513e-01 -1.3489e-01 -5.7239e-02\n",
      "  4.9241e-01  1.8643e-01  3.8596e-01 -3.7329e-02 -5.4216e-01 -1.8152e-01\n",
      "  4.3110e-01 -4.6967e-01  6.6801e-02  5.0323e-01 -2.4059e-01  3.6742e-01\n",
      "  2.9300e-01 -8.7883e-02 -4.7940e-01 -4.3431e-02 -2.6137e-01 -6.2658e-01\n",
      "  1.1446e-01  2.7682e-01  3.4800e-01  5.0018e-01  1.4269e-01 -3.3545e-01\n",
      " -3.9712e-01 -3.3121e-01 -3.4434e-01 -4.1627e-01 -3.5707e-03 -6.2350e-01\n",
      "  3.7794e-01 -1.6765e-01 -4.1954e-01 -3.3134e-01  3.1232e-01 -3.9494e-01\n",
      " -4.6921e-03 -4.8884e-01 -2.2059e-02 -2.6174e-01  1.7937e-01  3.6628e-01\n",
      "  5.8971e-02 -3.5991e-01 -4.4393e-01 -1.1890e-01  3.3487e-01  3.6505e-02\n",
      " -3.2788e-01  3.3425e-01 -5.6361e-01 -1.1190e-01  5.3770e-01  2.0311e-01\n",
      "  1.5110e-01  1.0623e-02  3.3401e-01  4.6084e-01  5.6293e-01 -7.5432e-02\n",
      "  5.4813e-01  1.9395e-01 -2.6265e-01 -3.1699e-01 -8.1778e-01  5.8169e-02\n",
      " -5.7866e-02 -1.1781e-01 -5.8742e-02 -1.4092e-01 -9.9394e-01 -9.4532e-02\n",
      "  2.3503e-01 -4.9027e-01  8.5832e-01  1.1540e-01 -1.5049e-01  1.9065e-01\n",
      " -2.6705e-01  2.5326e-01 -6.7579e-01 -1.0633e-02 -5.5158e-02 -3.1004e-01\n",
      " -5.8036e-02 -1.7200e-01  1.3298e-01 -3.2899e-01 -7.5481e-02  2.9425e-02\n",
      " -3.2949e-01 -1.8691e-01 -9.5323e-01 -3.5468e-01 -3.3162e-01  5.6441e-02\n",
      "  2.1790e-02  1.7182e-01 -4.4267e-01  6.9765e-01 -2.6876e-01  1.1659e-01\n",
      " -1.6584e-01  3.8296e-01  2.9109e-01  3.6318e-01  3.6961e-01  1.6305e-01\n",
      "  1.8152e-01  2.2453e-01  3.9866e-02 -3.7607e-02 -3.6089e-01  7.0818e-02\n",
      " -2.1509e-01  3.6551e-01 -5.1603e-01 -5.8102e-03 -4.8320e-01 -2.5068e-01\n",
      " -5.2062e-02 -2.0828e-01  2.9060e-01  2.2084e-02 -6.8123e-01  4.2063e-01\n",
      "  9.5973e-02  8.1720e-01 -1.5241e-01  6.2994e-01  2.6449e-01 -1.3516e-01\n",
      "  3.2450e-01  3.0503e-01  1.2357e-01  1.5107e-01  2.8327e-01 -3.3838e-01\n",
      "  4.6106e-02 -1.2361e-01  1.4516e-01 -2.7947e-02  2.6231e-02 -5.9591e-01\n",
      " -4.4183e-01  7.8440e-01 -3.4375e-02 -1.3928e+00  3.5248e-01  6.5220e-01]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_lg model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ✔ Well done! In the next exercise, you'll be using spaCy to predict\n",
    "    similarities between documents, spans and tokens via the word vectors under the\n",
    "    hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing similarities\n",
    "In this exercise, you’ll be using spaCy’s similarity methods to compare `Doc`, `Token` and `Span` objects and get similarity scores.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "    Use the doc.similarity method to compare doc1 to doc2 and print the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8789265574516525\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "    Use the token.similarity method to compare token1 to token2 and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2232533\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "    Create spans for “great restaurant”/“really nice bar”.\n",
    "    Use span.similarity to compare them and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7517392\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ✔ Well done! Feel free to experiment with comparing more objects, if\n",
    "    you like. The similarities are not *always* this conclusive. Once you're getting\n",
    "    serious about developing NLP applications that leverage semantic similarity, you\n",
    "    might want to train vectors on your own data, or tweak the similarity\n",
    "    algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining models and rules\n",
    "\n",
    "Combining statistical models with rules systems is powerful NLP tool\n",
    "\n",
    "## Statistical predictions vs. rules\n",
    "\n",
    "|                         | Statistical models | Rule-based systems |\n",
    "|:----------------------- | :----------------- | :----------------- |\n",
    "| __Use Cases__           | Application needs to _generalize_ based on examples | Dictionary with a finite number of examples |\n",
    "| __Real-world Examples__ | Product names. person names, subject/object relationships | countries of the world, cities, drug names, dog breeds |\n",
    "| __spaCy features__      | Entity recognizer, dependency parser, part-of-speech tagger | tokenizer, Matcher, PhraseMatcher |\n",
    "\n",
    "\n",
    "### Efficient phrase matching (1)\n",
    "- `PhraseMatcher` like regular expressions or keyword search - but with access to the tokens!\n",
    "- Takes `Doc` objects as patterns\n",
    "- More efficient and faster than the `Matcher`\n",
    "- Useful for matching large word lists\n",
    "\n",
    "### Efficient phrase matching (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "\n",
    "# Passing a Doc object as a new pattern using PhraseMatcher\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", [pattern])\n",
    "\n",
    "\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches in doc\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Return the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging patterns (1)\n",
    "\n",
    "Why does this pattern not match the tokens “Silicon Valley” in the `doc`?\n",
    "\n",
    "```python\n",
    "pattern = [{\"LOWER\": \"silicon\"}, {\"TEXT\": \" \"}, {\"LOWER\": \"valley\"}]\n",
    "\n",
    "doc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")\n",
    "```\n",
    "\n",
    "Answer: The tokenizer doesn’t create tokens for single spaces, so there’s no token with the value \" \" in between.\n",
    "\n",
    "That's correct!\n",
    "\n",
    "The tokenizer already takes care of splitting off whitespace and each dictionary in the pattern describes one token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging patters (2)\n",
    "\n",
    "Both patterns in this exercise contain mistakes and won’t match as expected. Can you fix them? If you get stuck, try printing the tokens in the doc to see how the text will be split and adjust the pattern so that each dictionary represents one token.\n",
    "\n",
    "    Edit pattern1 so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
    "    Edit pattern2 so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "\n",
    "# The hypen in `add-free` is a token in spaCy. It must be split into `ad`, `-`, and `free`\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient phrase matching\n",
    "\n",
    "Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world. We already have a list of countries, so let’s use this as the basis of our information extraction script. A list of string names is available as the variable `COUNTRIES`.\n",
    "\n",
    "    Import the PhraseMatcher and initialize it with the shared vocab as the variable matcher.\n",
    "    Add the phrase patterns and call the matcher on the doc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'exercises/en/countries.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-bdd441ef32f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exercises/en/countries.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mCOUNTRIES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'exercises/en/countries.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.____ import ____\n",
    "\n",
    "matcher = ____(____)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = ____(____)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES = ['Afghanistan',\n",
    " 'Åland Islands',\n",
    " 'Albania',\n",
    " 'Algeria',\n",
    " 'American Samoa',\n",
    " 'Andorra',\n",
    " 'Angola',\n",
    " 'Anguilla',\n",
    " 'Antarctica',\n",
    " 'Antigua and Barbuda',\n",
    " 'Argentina',\n",
    " 'Armenia',\n",
    " 'Aruba',\n",
    " 'Australia',\n",
    " 'Austria',\n",
    " 'Azerbaijan',\n",
    " 'Bahamas',\n",
    " 'Bahrain',\n",
    " 'Bangladesh',\n",
    " 'Barbados',\n",
    " 'Belarus',\n",
    " 'Belgium',\n",
    " 'Belize',\n",
    " 'Benin',\n",
    " 'Bermuda',\n",
    " 'Bhutan',\n",
    " 'Bolivia (Plurinational State of)',\n",
    " 'Bonaire, Sint Eustatius and Saba',\n",
    " 'Bosnia and Herzegovina',\n",
    " 'Botswana',\n",
    " 'Bouvet Island',\n",
    " 'Brazil',\n",
    " 'British Indian Ocean Territory',\n",
    " 'United States Minor Outlying Islands',\n",
    " 'Virgin Islands (British)',\n",
    " 'Virgin Islands (U.S.)',\n",
    " 'Brunei Darussalam',\n",
    " 'Bulgaria',\n",
    " 'Burkina Faso',\n",
    " 'Burundi',\n",
    " 'Cambodia',\n",
    " 'Cameroon',\n",
    " 'Canada',\n",
    " 'Cabo Verde',\n",
    " 'Cayman Islands',\n",
    " 'Central African Republic',\n",
    " 'Chad',\n",
    " 'Chile',\n",
    " 'China',\n",
    " 'Christmas Island',\n",
    " 'Cocos (Keeling) Islands',\n",
    " 'Colombia',\n",
    " 'Comoros',\n",
    " 'Congo',\n",
    " 'Congo (Democratic Republic of the)',\n",
    " 'Cook Islands',\n",
    " 'Costa Rica',\n",
    " 'Croatia',\n",
    " 'Cuba',\n",
    " 'Curaçao',\n",
    " 'Cyprus',\n",
    " 'Czech Republic',\n",
    " 'Denmark',\n",
    " 'Djibouti',\n",
    " 'Dominica',\n",
    " 'Dominican Republic',\n",
    " 'Ecuador',\n",
    " 'Egypt',\n",
    " 'El Salvador',\n",
    " 'Equatorial Guinea',\n",
    " 'Eritrea',\n",
    " 'Estonia',\n",
    " 'Ethiopia',\n",
    " 'Falkland Islands (Malvinas)',\n",
    " 'Faroe Islands',\n",
    " 'Fiji',\n",
    " 'Finland',\n",
    " 'France',\n",
    " 'French Guiana',\n",
    " 'French Polynesia',\n",
    " 'French Southern Territories',\n",
    " 'Gabon',\n",
    " 'Gambia',\n",
    " 'Georgia',\n",
    " 'Germany',\n",
    " 'Ghana',\n",
    " 'Gibraltar',\n",
    " 'Greece',\n",
    " 'Greenland',\n",
    " 'Grenada',\n",
    " 'Guadeloupe',\n",
    " 'Guam',\n",
    " 'Guatemala',\n",
    " 'Guernsey',\n",
    " 'Guinea',\n",
    " 'Guinea-Bissau',\n",
    " 'Guyana',\n",
    " 'Haiti',\n",
    " 'Heard Island and McDonald Islands',\n",
    " 'Holy See',\n",
    " 'Honduras',\n",
    " 'Hong Kong',\n",
    " 'Hungary',\n",
    " 'Iceland',\n",
    " 'India',\n",
    " 'Indonesia',\n",
    " \"Côte d'Ivoire\",\n",
    " 'Iran (Islamic Republic of)',\n",
    " 'Iraq',\n",
    " 'Ireland',\n",
    " 'Isle of Man',\n",
    " 'Israel',\n",
    " 'Italy',\n",
    " 'Jamaica',\n",
    " 'Japan',\n",
    " 'Jersey',\n",
    " 'Jordan',\n",
    " 'Kazakhstan',\n",
    " 'Kenya',\n",
    " 'Kiribati',\n",
    " 'Kuwait',\n",
    " 'Kyrgyzstan',\n",
    " \"Lao People's Democratic Republic\",\n",
    " 'Latvia',\n",
    " 'Lebanon',\n",
    " 'Lesotho',\n",
    " 'Liberia',\n",
    " 'Libya',\n",
    " 'Liechtenstein',\n",
    " 'Lithuania',\n",
    " 'Luxembourg',\n",
    " 'Macao',\n",
    " 'Macedonia (the former Yugoslav Republic of)',\n",
    " 'Madagascar',\n",
    " 'Malawi',\n",
    " 'Malaysia',\n",
    " 'Maldives',\n",
    " 'Mali',\n",
    " 'Malta',\n",
    " 'Marshall Islands',\n",
    " 'Martinique',\n",
    " 'Mauritania',\n",
    " 'Mauritius',\n",
    " 'Mayotte',\n",
    " 'Mexico',\n",
    " 'Micronesia (Federated States of)',\n",
    " 'Moldova (Republic of)',\n",
    " 'Monaco',\n",
    " 'Mongolia',\n",
    " 'Montenegro',\n",
    " 'Montserrat',\n",
    " 'Morocco',\n",
    " 'Mozambique',\n",
    " 'Myanmar',\n",
    " 'Namibia',\n",
    " 'Nauru',\n",
    " 'Nepal',\n",
    " 'Netherlands',\n",
    " 'New Caledonia',\n",
    " 'New Zealand',\n",
    " 'Nicaragua',\n",
    " 'Niger',\n",
    " 'Nigeria',\n",
    " 'Niue',\n",
    " 'Norfolk Island',\n",
    " \"Korea (Democratic People's Republic of)\",\n",
    " 'Northern Mariana Islands',\n",
    " 'Norway',\n",
    " 'Oman',\n",
    " 'Pakistan',\n",
    " 'Palau',\n",
    " 'Palestine, State of',\n",
    " 'Panama',\n",
    " 'Papua New Guinea',\n",
    " 'Paraguay',\n",
    " 'Peru',\n",
    " 'Philippines',\n",
    " 'Pitcairn',\n",
    " 'Poland',\n",
    " 'Portugal',\n",
    " 'Puerto Rico',\n",
    " 'Qatar',\n",
    " 'Republic of Kosovo',\n",
    " 'Réunion',\n",
    " 'Romania',\n",
    " 'Russian Federation',\n",
    " 'Rwanda',\n",
    " 'Saint Barthélemy',\n",
    " 'Saint Helena, Ascension and Tristan da Cunha',\n",
    " 'Saint Kitts and Nevis',\n",
    " 'Saint Lucia',\n",
    " 'Saint Martin (French part)',\n",
    " 'Saint Pierre and Miquelon',\n",
    " 'Saint Vincent and the Grenadines',\n",
    " 'Samoa',\n",
    " 'San Marino',\n",
    " 'Sao Tome and Principe',\n",
    " 'Saudi Arabia',\n",
    " 'Senegal',\n",
    " 'Serbia',\n",
    " 'Seychelles',\n",
    " 'Sierra Leone',\n",
    " 'Singapore',\n",
    " 'Sint Maarten (Dutch part)',\n",
    " 'Slovakia',\n",
    " 'Slovenia',\n",
    " 'Solomon Islands',\n",
    " 'Somalia',\n",
    " 'South Africa',\n",
    " 'South Georgia and the South Sandwich Islands',\n",
    " 'Korea (Republic of)',\n",
    " 'South Sudan',\n",
    " 'Spain',\n",
    " 'Sri Lanka',\n",
    " 'Sudan',\n",
    " 'Suriname',\n",
    " 'Svalbard and Jan Mayen',\n",
    " 'Swaziland',\n",
    " 'Sweden',\n",
    " 'Switzerland',\n",
    " 'Syrian Arab Republic',\n",
    " 'Taiwan',\n",
    " 'Tajikistan',\n",
    " 'Tanzania, United Republic of',\n",
    " 'Thailand',\n",
    " 'Timor-Leste',\n",
    " 'Togo',\n",
    " 'Tokelau',\n",
    " 'Tonga',\n",
    " 'Trinidad and Tobago',\n",
    " 'Tunisia',\n",
    " 'Turkey',\n",
    " 'Turkmenistan',\n",
    " 'Turks and Caicos Islands',\n",
    " 'Tuvalu',\n",
    " 'Uganda',\n",
    " 'Ukraine',\n",
    " 'United Arab Emirates',\n",
    " 'United Kingdom of Great Britain and Northern Ireland',\n",
    " 'United States of America',\n",
    " 'Uruguay',\n",
    " 'Uzbekistan',\n",
    " 'Vanuatu',\n",
    " 'Venezuela (Bolivarian Republic of)',\n",
    " 'Viet Nam',\n",
    " 'Wallis and Futuna',\n",
    " 'Western Sahara',\n",
    " 'Yemen',\n",
    " 'Zambia',\n",
    " 'Zimbabwe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", *[patterns])\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ✔ Well done! Let's use this matcher to add some custom entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting countries and relationships\n",
    "\n",
    "In the previous exercise, you wrote a script using spaCy’s `PhraseMatcher` to find country names in text. Let’s use that country matcher on a longer text, analyze the syntax and update the document’s entities with the matched countries.\n",
    "\n",
    "    Iterate over the matches and create a Span with the label \"GPE\" (geopolitical entity).\n",
    "    Overwrite the entities in doc.ents and add the matched span.\n",
    "    Get the matched span’s root head token.\n",
    "    Print the text of the head token and the span.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = 'After the Cold War, the UN saw a radical expansion in its peacekeeping duties, taking on more missions in ten years than it had in the previous four decades.Between 1988 and 2000, the number of adopted Security Council resolutions more than doubled, and the peacekeeping budget increased more than tenfold. The UN negotiated an end to the Salvadoran Civil War, launched a successful peacekeeping mission in Namibia, and oversaw democratic elections in post-apartheid South Africa and post-Khmer Rouge Cambodia. In 1991, the UN authorized a US-led coalition that repulsed the Iraqi invasion of Kuwait. Brian Urquhart, Under-Secretary-General from 1971 to 1985, later described the hopes raised by these successes as a \"false renaissance\" for the organization, given the more troubled missions that followed. Though the UN Charter had been written primarily to prevent aggression by one nation against another, in the early 1990s the UN faced a number of simultaneous, serious crises within nations such as Somalia, Haiti, Mozambique, and the former Yugoslavia. The UN mission in Somalia was widely viewed as a failure after the US withdrawal following casualties in the Battle of Mogadishu, and the UN mission to Bosnia faced \"worldwide ridicule\" for its indecisive and confused mission in the face of ethnic cleansing. In 1994, the UN Assistance Mission for Rwanda failed to intervene in the Rwandan genocide amid indecision in the Security Council. Beginning in the last decades of the Cold War, American and European critics of the UN condemned the organization for perceived mismanagement and corruption. In 1984, the US President, Ronald Reagan, withdrew his nation\\'s funding from UNESCO (the United Nations Educational, Scientific and Cultural Organization, founded 1946) over allegations of mismanagement, followed by Britain and Singapore. Boutros Boutros-Ghali, Secretary-General from 1992 to 1996, initiated a reform of the Secretariat, reducing the size of the organization somewhat. His successor, Kofi Annan (1997–2006), initiated further management reforms in the face of threats from the United States to withhold its UN dues. In the late 1990s and 2000s, international interventions authorized by the UN took a wider variety of forms. The UN mission in the Sierra Leone Civil War of 1991–2002 was supplemented by British Royal Marines, and the invasion of Afghanistan in 2001 was overseen by NATO. In 2003, the United States invaded Iraq despite failing to pass a UN Security Council resolution for authorization, prompting a new round of questioning of the organization\\'s effectiveness. Under the eighth Secretary-General, Ban Ki-moon, the UN has intervened with peacekeepers in crises including the War in Darfur in Sudan and the Kivu conflict in the Democratic Republic of Congo and sent observers and chemical weapons inspectors to the Syrian Civil War. In 2013, an internal review of UN actions in the final battles of the Sri Lankan Civil War in 2009 concluded that the organization had suffered \"systemic failure\". One hundred and one UN personnel died in the 2010 Haiti earthquake, the worst loss of life in the organization\\'s history. The Millennium Summit was held in 2000 to discuss the UN\\'s role in the 21st century. The three day meeting was the largest gathering of world leaders in history, and culminated in the adoption by all member states of the Millennium Development Goals (MDGs), a commitment to achieve international development in areas such as poverty reduction, gender equality, and public health. Progress towards these goals, which were to be met by 2015, was ultimately uneven. The 2005 World Summit reaffirmed the UN\\'s focus on promoting development, peacekeeping, human rights, and global security. The Sustainable Development Goals were launched in 2015 to succeed the Millennium Development Goals. In addition to addressing global challenges, the UN has sought to improve its accountability and democratic legitimacy by engaging more with civil society and fostering a global constituency. In an effort to enhance transparency, in 2016 the organization held its first public debate between candidates for Secretary-General. On 1 January 2017, Portuguese diplomat António Guterres, who previously served as UN High Commissioner for Refugees, became the ninth Secretary-General. Guterres has highlighted several key goals for his administration, including an emphasis on diplomacy for preventing conflicts, more effective peacekeeping efforts, and streamlining the organization to be more responsive and versatile to global needs.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Creating a loop add each country into the matcher object\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", *[patterns])\n",
    "\n",
    "# Create a doc and reset existing entities\n",
    "doc = nlp(TEXT)\n",
    "\n",
    "# Reset entities and manually create GPE entities\n",
    "doc.ents = []\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = nlp.vocab['love']\n",
    "\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is awesome!\n",
    "```python\n",
    "dir(lexeme)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love black coffee, from the hearts mountains of Costa Rica.\")\n",
    "\n",
    "for i in doc.noun_chunks:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
