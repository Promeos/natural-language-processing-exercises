{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Large-scale data analysis with spaCy\n",
    "\n",
    "https://course.spacy.io/en/chapter2\n",
    "\n",
    "In this chapter, you'll use your new skills to extract specific information from large volumes of text. You''ll learn how to make the most of spaCy's data structures, and how to effectively combine statistical and rule-based approaches for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structures Part 1\n",
    "## Vocab, Lexemes, and StringStore\n",
    "\n",
    "### Shared Vocab and StringStore Part 1\n",
    "- spaCy stores shared strings/tokens/data across multiple documents.\n",
    "- spaCy saves memory by encoding all strings to hash values.\n",
    "- Strings are only stored once in the `StringStore` via `nlp.vocab.strings`\n",
    "- String store: lookup table in both directions.\n",
    "    - Passing a string returns a hash value\n",
    "    ```python\n",
    "    # Hash value\n",
    "    earth_hash = nlp.vocab.strings['Earth']\n",
    "    ```\n",
    "    - Passing a hash value returns a string\n",
    "    ```python\n",
    "    # String value\n",
    "    nlp.vocab.strings[earth_hash]\n",
    "    ```\n",
    "- Hashes cannot be reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "=============== Installed pipeline packages (spaCy v3.0.0rc5) ===============\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME             SPACY               VERSION                              \n",
      "en_core_web_md   >=3.0.0rc3,<3.1.0   \u001b[38;5;2m3.0.0a1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "en_core_web_sm   >=3.0.0rc3,<3.1.0   \u001b[38;5;2m3.0.0a1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "en_core_web_lg   >=3.0.0rc3,<3.1.0   \u001b[38;5;2m3.0.0a1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10533021089177626446"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "nlp.vocab.strings['Earth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "spacy 2.3.5 requires catalogue<1.1.0,>=0.0.7, but you have catalogue 2.0.1 which is incompatible.\n",
    "spacy 2.3.5 requires srsly<1.1.0,>=1.0.2, but you have srsly 2.3.2 which is incompatible.\n",
    "spacy 2.3.5 requires thinc<7.5.0,>=7.4.1, but you have thinc 8.0.1 which is incompatible.\n",
    "spacy-transformers 1.0.0rc0 requires transformers<3.1.0,>=3.0.0, but you have transformers 4.2.2 which is incompatible.\n",
    "allennlp-models 1.0.0 requires allennlp==1.0.0, but you have allennlp 2.0.1 which is incompatible.\n",
    "\n",
    "pip install catalogue==1.0.0\n",
    "pip install srsly==1.0.2\n",
    "pip install thinc==7.4.1\n",
    "pip install transformers==3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nlp.vocab.strings[hash_value]` will raise an error because the nlp object __has not seen the hash value of Earth__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Earth'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.strings[10533021089177626446]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Always pass around the shared vocab between a doc and the nlp object__\n",
    "\n",
    "To use the string and hash value as inputs in `nlp.vocab.string[input]` we need to give the nlp object text that contains the word we're trying to look up with it's hash value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Earth'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I live on Earth. It's a beautiful planet with diverse life forms.\" \\\n",
    "          \"Over millions of years, these creatures have adapted to harsh climates\")\n",
    "\n",
    "# The nlp object has `memory` of the word 'Earth' an successfully returns the string, given its hash value.\n",
    "nlp.vocab.strings[10533021089177626446]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Vocab and String Store Part 2\n",
    "You can use the __nlp object__ and the __doc object__ to look up the string value or hash value of a token.\n",
    "\n",
    "#### Find the string and hash values using the nlp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash value: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love black coffee, from the hearts mountains of Costa Rica.\")\n",
    "\n",
    "# Display the hash value of the string \"coffee\"\n",
    "print(\"Hash value:\", nlp.vocab.strings['coffee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String value: coffee\n"
     ]
    }
   ],
   "source": [
    "# Display the string of the hash value 3197928453018144401\n",
    "print(\"String value:\", nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the string and hash values using the doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash value: 3197928453018144401\n",
      "String value: coffee\n"
     ]
    }
   ],
   "source": [
    "print(\"Hash value:\", doc.vocab.strings['coffee'])\n",
    "print(\"String value:\", doc.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexemes: entries in the vocabulary\n",
    "A `Lexeme` object is an entry in the vocabulary. It contains the __context-independent__ information about a word.\n",
    "- Word text: lexeme.text for the string and lexeme.orth for the hash value\n",
    "- Lexical attributes of the string, e.g. lexeme.is_alpha\n",
    "- Lexemes DO NOT contain Parts-of-speech tags, dependencies, or entity labels. These attributes depend on the __CONTEXT__ of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
